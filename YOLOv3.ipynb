{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import copy\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import unittest\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.ops import nms, box_iou, box_convert\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdp_rh03TzlV"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bsmMjUEYTyKe"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.BB_TRESHOLD = 50 #defines whats considered a large boundingbox for the filtering of the data\n",
    "    self.dataSplit = [0.7, 0.1, 0.2] #train, evaluation, test\n",
    "    self.numOfIm = 12880 + 3226 -1 #total num of images in dataset.\n",
    "    self.DATA_PATH = '../wider_face/'\n",
    "    self.INDEX_PATH_TRAIN = self.DATA_PATH + 'annotation/wider_face_train_bbx_gt.txt'\n",
    "    self.INDEX_PATH_VAL = self.DATA_PATH + 'annotation/wider_face_val_bbx_gt.txt'\n",
    "    self.INDEX_PATH_ALL = self.DATA_PATH + 'annotation/wider_face_all.txt'\n",
    "    self.IMAGE_STORE_PATH_TRAIN = self.DATA_PATH + 'train/'\n",
    "    self.IMAGE_STORE_PATH_VAL = self.DATA_PATH + 'validation/'\n",
    "    self.ANCHORS = [\n",
    "      [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "      [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "      [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "    ]  # Note these have been rescaled to be between [0, 1]\n",
    "  \n",
    "\n",
    "    #TODO replace these by what we use\n",
    "    self.DATASET = '/PASCAL_VOC'\n",
    "    self.IMG_DIR = '/PASCAL_VOC/images/'\n",
    "    self.LABEL_DIR = '/PASCAL_VOC/labels/'\n",
    "    self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # UTILS.seed_everything()  # If you want deterministic behavior\n",
    "    self.NUM_WORKERS = 0 #4 causes errors on windows to have more than 0\n",
    "    self.BATCH_SIZE = 10\n",
    "    self.IMAGE_SIZE = 416\n",
    "    self.LEARNING_RATE = 1e-4\n",
    "    self.WEIGHT_DECAY = 1e-4 #staat uit nu om te overfitten, kan aan tijdens echte trainen\n",
    "    self.NUM_EPOCHS = 100\n",
    "    self.CONF_THRESHOLD = 0.7#0.05\n",
    "    self.MAP_IOU_THRESH = 0.25#0.5\n",
    "    self.NMS_IOU_THRESH = 0.25#0.45\n",
    "    self.S = [self.IMAGE_SIZE // 32, self.IMAGE_SIZE // 16, self.IMAGE_SIZE // 8]\n",
    "    self.PIN_MEMORY = True\n",
    "    self.LOAD_CHECKPOINT = True\n",
    "    self.SAVE_MODEL = True\n",
    "    self.SAVE_RESULTS = True\n",
    "    self.CHECKPOINT_PATH = \"../checkpoints/\"\n",
    "    self.RESULTS_PATH = \"results/\"\n",
    "    self.TINY_MAP_AREA = 0.01\n",
    "    \n",
    "    self.ANCHORS = [\n",
    "        [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "        [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "        [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "    ]  # Note these have been rescaled to be between [0, 1]\n",
    "    \n",
    "    self.scale = 1.1\n",
    "    self.train_transforms = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=int(self.IMAGE_SIZE * self.scale)),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=int(self.IMAGE_SIZE * self.scale),\n",
    "                min_width=int(self.IMAGE_SIZE * self.scale),\n",
    "                border_mode=cv2.BORDER_CONSTANT,\n",
    "            ),\n",
    "            A.RandomCrop(width=self.IMAGE_SIZE, height=self.IMAGE_SIZE),\n",
    "            A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Blur(p=0.1),\n",
    "            A.CLAHE(p=0.1),\n",
    "            A.Posterize(p=0.1),\n",
    "            A.ToGray(p=0.1),\n",
    "            A.ChannelShuffle(p=0.05),\n",
    "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format=\"coco\", min_visibility=0.4, label_fields=[],),\n",
    "    )\n",
    "\n",
    "    self.test_transforms = A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=self.IMAGE_SIZE),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=self.IMAGE_SIZE, min_width=self.IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "            ),\n",
    "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format=\"coco\", min_visibility=0.4, label_fields=[]),\n",
    "    )\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# YOLOv3 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of YOLOv3 architecture\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride)\n",
    "Every conv is a same convolution.\n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],  # To this point is Darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
    "                              bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels,\n",
    "                             kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, 5 * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "# This module applies the math opperations from the YOLO paper to retrieve the true coordinates and with and height of the bboxes instead of doing this at a later stage.\n",
    "\n",
    "\n",
    "class MathOperations(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.anchors = (torch.tensor(CONFIG.ANCHORS) * torch.tensor(\n",
    "            CONFIG.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)).to(CONFIG.DEVICE)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x is in the shape N x 3 x S x S x 5\n",
    "    we want the last dimension to be in this order: [x, y, w, h, obj]\n",
    "    In the forward we'll apply the mathematical operations described in both YOLO 9000 and YOLO V3 papers\n",
    "\n",
    "    bx = sigma(tx) + cx\n",
    "    by = sigma(ty) + cy\n",
    "    bw = pw * exp(tw)\n",
    "    bh = ph * exp(th)\n",
    "\n",
    "    cx/cy = location of upper left corner of cell\n",
    "    pw/ph = prior size of the anchor box\n",
    "    t = vector before operation\n",
    "    b = vector after operation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, scale):\n",
    "        # return  torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "        return torch.cat([self.sigmoid(x[..., 0:2]), torch.exp(x[..., 2:4]) * self.anchors[scale].reshape(1, 3, 1, 1, 2), self.sigmoid(x[..., 4:5])], dim=-1)\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        MathOperationsLayer = MathOperations()\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(MathOperationsLayer(layer(x), len(outputs)))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(\n",
    "                    in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(\n",
    "                            in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CcNuKTNUCxn"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "IqeOM9HNUEug"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....................\n",
      "----------------------------------------------------------------------\n",
      "Ran 21 tests in 3.230s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x22b84ad14c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Utils():\n",
    "\n",
    "    def get_precision_recall(self, pred_boxes, true_boxes,  filter_tiny, filter_area, iou_threshold=CONFIG.MAP_IOU_THRESH):\n",
    "        \"\"\"\n",
    "        Video explanation of this function:\n",
    "        https://youtu.be/FppOzcDvaDI\n",
    "        This function calculates mean average precision (mAP)\n",
    "        Parameters:\n",
    "            pred_boxes (list): list of list of lists containing all bboxes with each bboxes\n",
    "            specified as [cx,cy,w,h,conf_score] for each training example\n",
    "            true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "            iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        Returns:\n",
    "            float: mAP value across all classes given a specific IoU threshold\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert list of lists of bounding boxes per sample to one list of\n",
    "        # bounding boxes with the sample as item of a bounding box.\n",
    "        # This allows sorting based on confidence level\n",
    "        # the shape is now [sample_idx, cx, xy, w, h, conf]\n",
    "        pred_boxes = [[sample_idx] + bbox for sample_idx,\n",
    "                    sample in enumerate(pred_boxes) for bbox in sample]\n",
    "        true_boxes = [[sample_idx] + bbox[:4] for sample_idx,\n",
    "                    sample in enumerate(true_boxes) for bbox in sample]\n",
    "        \n",
    "        #filter the bboxes based on their area to be able to calculate mAP for just smal boxes\n",
    "        if filter_tiny:\n",
    "            pred_boxes = [box for box in pred_boxes if box[3] * box[4] <= filter_area]\n",
    "            true_boxes = [box for box in true_boxes if box[3] * box[4] <= filter_area]\n",
    "\n",
    "        # Tensor to keep track of already found bounding boxes.\n",
    "        found = torch.zeros(len(true_boxes))\n",
    "\n",
    "        # Sort detections based on confidence, this is the ranking for mAP\n",
    "        pred_boxes.sort(key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "        # Make list of TP and FP to keep track of them for each step\n",
    "        TP = torch.zeros(len(pred_boxes))\n",
    "        FP = torch.zeros(len(pred_boxes))\n",
    "\n",
    "        # Loop trough the sorted detections starting from highest confidence (rank)\n",
    "        for rank, detection in enumerate(pred_boxes):\n",
    "\n",
    "            # Find all possible ground_truths for this detection\n",
    "            ground_truth = []\n",
    "            ground_truth_id = []\n",
    "            for id, bbox in enumerate(true_boxes):\n",
    "                if bbox[0] == detection[0] and not found[id]:\n",
    "                    ground_truth.append(bbox[-4:])\n",
    "                    ground_truth_id.append(id)\n",
    "\n",
    "            ground_truth = torch.tensor(ground_truth)\n",
    "\n",
    "            # Add dimension to detection for the torchvision box_iou function\n",
    "            detection = torch.Tensor(detection[1:-1]).unsqueeze(0)\n",
    "\n",
    "            # if there is no ground truth, false positive by default\n",
    "            if ground_truth.shape[0] <= 0:\n",
    "                FP[rank] = 1\n",
    "                continue\n",
    "\n",
    "            IOU = UTILS.intersection_over_union(detection, ground_truth)\n",
    "\n",
    "            best_iou = torch.max(IOU)\n",
    "            best_iou_idx = torch.argmax(IOU)\n",
    "\n",
    "            if best_iou >= iou_threshold:\n",
    "                found[ground_truth_id[best_iou_idx]] = 1\n",
    "                TP[rank] = 1\n",
    "            else:\n",
    "                FP[rank] = 1\n",
    "\n",
    "        # comulative sum of tp and fp to be able to calculate precision and recall at each rank\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "\n",
    "        # calculate recall and precision at each rank\n",
    "        recalls = TP_cumsum / len(true_boxes)  # TP / (TP + FN)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum)  # TP / TP + FP\n",
    "\n",
    "        # add 1 to precision because before rank 0 the precision is 100%\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "\n",
    "        # add 0 to recalls because before rank 0 the recall is 0%\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "\n",
    "        return precisions, recalls\n",
    "\n",
    "    def mean_average_precision(self, pred_boxes, true_boxes, iou_threshold=CONFIG.MAP_IOU_THRESH, tiny=False, filter_area=CONFIG.MAP_IOU_THRESH):\n",
    "\n",
    "        precisions, recalls = self.get_precision_recall(\n",
    "            pred_boxes=pred_boxes, true_boxes=true_boxes, iou_threshold=iou_threshold, filter_tiny=tiny, filter_area=filter_area)\n",
    "\n",
    "        # get area under precision-recall graph using trapezoid rule\n",
    "        mAP = torch.trapz(precisions, recalls)\n",
    "        return mAP.item()\n",
    "\n",
    "    def intersection_over_union(self, bboxes1, bboxes2, format=\"cxcywh\"):\n",
    "        \"\"\"\n",
    "            This function calculates the intersection over union of two sets of\n",
    "            bounding boxes. It takes the two tensors of bounding boxes and a \n",
    "            string representing their format as input. With the formats being\n",
    "            defined as the following by torchvision:\n",
    "\n",
    "            ‘xyxy’: boxes are represented via corners, x1, y1 being top left and \n",
    "                    x2, y2 being bottom right. This is the format that torchvision \n",
    "                    utilities expect.\n",
    "\n",
    "            ‘xywh’: boxes are represented via corner, width and height, x1, y2 \n",
    "                    being top left, w, h being width and height.\n",
    "\n",
    "            ‘cxcywh’: boxes are represented via centre, width and height, cx, cy \n",
    "                    being center of box, w, h being width and height.\n",
    "\n",
    "        \"\"\"\n",
    "        assert type(bboxes1) == torch.Tensor and type(\n",
    "            bboxes2) == torch.Tensor, \"One or both of the bboxes is not a torch.Tensor\"\n",
    "        bboxes1.to(CONFIG.DEVICE)\n",
    "        bboxes2.to(CONFIG.DEVICE)\n",
    "\n",
    "        if (format == \"cxcywh\"):\n",
    "            return box_iou(box_convert(bboxes1, \"cxcywh\", \"xyxy\"), box_convert(bboxes2, \"cxcywh\", \"xyxy\"))\n",
    "        elif (format == \"xywh\"):\n",
    "            return box_iou(box_convert(bboxes1, \"xywh\", \"xyxy\"), box_convert(bboxes2, \"xywh\", \"xyxy\"))\n",
    "        else:  # format == \"xyxy\"\n",
    "            return box_iou(bboxes1, bboxes2)\n",
    "\n",
    "    def DIoU_penalty(self, bboxes1, bboxes2, format=\"cxcywh\"):\n",
    "        # Denoted in https://arxiv.org/pdf/1911.08287.pdf as R_DIoU, called the \"penalty term\" (equation 6).\n",
    "\n",
    "        if type(bboxes1) == list:\n",
    "            bboxes1 = torch.tensor(bboxes1)\n",
    "        if type(bboxes2) == list:\n",
    "            bboxes2 = torch.tensor(bboxes2)\n",
    "\n",
    "        # \"rho\" is the Euclidian (2D) distance between the center points of the target and predicted boxes.\n",
    "        rho = torch.dist(bboxes1.squeeze(), bboxes2.squeeze())\n",
    "\n",
    "        # \"c\" is the diagonal length of the smallest box that encloses both bounding boxes.\n",
    "        b1xyxy = box_convert(bboxes1, \"cxcywh\", \"xyxy\").squeeze()\n",
    "        b2xyxy = box_convert(bboxes2, \"cxcywh\", \"xyxy\").squeeze()\n",
    "\n",
    "        top_left_enclosing_box = torch.hstack(\n",
    "            [torch.minimum(b1xyxy[..., 0], b2xyxy[..., 0]), torch.minimum(b1xyxy[..., 1], b2xyxy[..., 1])])\n",
    "\n",
    "        bottom_right_enclosing_box = torch.hstack(\n",
    "            [torch.maximum(b1xyxy[..., 2], b2xyxy[..., 2]), torch.maximum(b1xyxy[..., 3], b2xyxy[..., 3])])\n",
    "\n",
    "        c = torch.dist(top_left_enclosing_box, bottom_right_enclosing_box)\n",
    "\n",
    "        return torch.square(rho)/torch.square(c)  # Penalty factor R_DIoU.\n",
    "\n",
    "    def distance_intersection_over_union_loss(self, bboxes1, bboxes2):\n",
    "        # Equation 7 in https://arxiv.org/pdf/1911.08287.pdf.\n",
    "        if type(bboxes1) == list:\n",
    "            bboxes1 = torch.tensor(bboxes1)\n",
    "        if type(bboxes2) == list:\n",
    "            bboxes2 = torch.tensor(bboxes2)\n",
    "\n",
    "        R_DIoU = self.DIoU_penalty(bboxes1, bboxes2)\n",
    "        IOU = self.intersection_over_union(bboxes1, bboxes2).diagonal()\n",
    "        DIoU_loss = 1 - IOU + R_DIoU\n",
    "        return DIoU_loss\n",
    "\n",
    "    def CIoU_penalty(self, predicted, ground_truth):\n",
    "        # Equation 8 in https://arxiv.org/pdf/1911.08287.pdf.\n",
    "\n",
    "        if type(predicted) == list:\n",
    "            predicted = torch.tensor(predicted)\n",
    "        if type(ground_truth) == list:\n",
    "            ground_truth = torch.tensor(ground_truth)\n",
    "\n",
    "        w_gt = ground_truth[..., 2]  # Width of the ground truth box.\n",
    "        h_gt = ground_truth[..., 3]  # Height of the ground truth box.\n",
    "        w_pred = predicted[..., 2]  # Width of the predicted box.\n",
    "        h_pred = predicted[..., 3]  # Height of the predicted box.\n",
    "\n",
    "        # Equation 9\n",
    "        v = 4/np.pi * \\\n",
    "            torch.square((torch.arctan(w_gt/h_gt) -\n",
    "                         torch.arctan(w_pred/h_pred)))\n",
    "\n",
    "        IOU = self.intersection_over_union(predicted, ground_truth).diagonal()\n",
    "\n",
    "        # An IOU of 1 gives v/(1-1+v) = v/v, but a perfect overlap also gives v=0, so 0/0.\n",
    "        # Multiplying v=0 with alpha also gives 0, so when 'nan' is produced, just replace it\n",
    "        # with R_DIoU.\n",
    "        \n",
    "        alpha = v / (1 - IOU + v)  # Equation 11\n",
    "\n",
    "        R_DIoU = self.DIoU_penalty(predicted, ground_truth)  # Equation 10\n",
    "        penalty = R_DIoU + (alpha * v)\n",
    "        penalty = torch.nan_to_num(penalty, nan=R_DIoU)\n",
    "        return penalty\n",
    "\n",
    "    def complete_intersection_over_union_loss(self, bboxes1, bboxes2):\n",
    "        if type(bboxes1) == list:\n",
    "            bboxes1 = torch.tensor(bboxes1)\n",
    "        if type(bboxes2) == list:\n",
    "            bboxes2 = torch.tensor(bboxes2)\n",
    "\n",
    "        R_CIoU = self.CIoU_penalty(bboxes1, bboxes2)\n",
    "        IOU = self.intersection_over_union(bboxes1, bboxes2).diagonal()\n",
    "        CIoU_loss = 1 - IOU + R_CIoU\n",
    "        return CIoU_loss\n",
    "\n",
    "    def generalised_intersection_over_union_loss(self, bboxes1, bboxes2):\n",
    "        if type(bboxes1) == list:\n",
    "            bboxes1 = torch.tensor(bboxes1)\n",
    "        if type(bboxes2) == list:\n",
    "            bboxes2 = torch.tensor(bboxes2)\n",
    "\n",
    "        b1xyxy = box_convert(bboxes1, \"cxcywh\", \"xyxy\").squeeze()\n",
    "        b2xyxy = box_convert(bboxes2, \"cxcywh\", \"xyxy\").squeeze()\n",
    "\n",
    "        # C is the smallest box that encloses both the target and the predicted box.\n",
    "\n",
    "        # Step 1 in https://tinyurl.com/giouloss Algorithm 1\n",
    "        C_xmin = torch.min(b1xyxy[..., 0], b2xyxy[..., 0])\n",
    "        C_ymin = torch.min(b1xyxy[..., 1], b2xyxy[..., 1])\n",
    "        C_xmax = torch.max(b1xyxy[..., 2], b2xyxy[..., 2])\n",
    "        C_ymax = torch.max(b1xyxy[..., 3], b2xyxy[..., 3])\n",
    "        # C_xyxy = torch.tensor([C_xmin, C_ymin, C_xmax, C_ymax])\n",
    "        # C = box_convert(C_xyxy, \"xyxy\", \"cxcywh\")\n",
    "\n",
    "        # Step 2 in https://tinyurl.com/giouloss Algorithm 1\n",
    "        iou = self.intersection_over_union(bboxes1, bboxes2)\n",
    "\n",
    "        # Step 3 in https://tinyurl.com/giouloss Algorithm 1\n",
    "        # Calculate |A union B|\n",
    "        intersection_xmin = torch.max(b1xyxy[..., 0], b2xyxy[..., 0])\n",
    "        intersection_ymin = torch.max(b1xyxy[..., 1], b2xyxy[..., 1])\n",
    "        intersection_xmax = torch.min(b1xyxy[..., 2], b2xyxy[..., 2])\n",
    "        intersection_ymax = torch.min(b1xyxy[..., 3], b2xyxy[..., 3])\n",
    "\n",
    "        intersection_width = intersection_xmax - intersection_xmin\n",
    "        intersection_height = intersection_ymax - intersection_ymin\n",
    "        intersection_area = intersection_width * intersection_height\n",
    "\n",
    "        area_A = (b1xyxy[..., 2] - b1xyxy[..., 0]) * \\\n",
    "            (b1xyxy[..., 3] - b1xyxy[..., 1])\n",
    "        area_B = (b2xyxy[..., 2] - b2xyxy[..., 0]) * \\\n",
    "            (b2xyxy[..., 3] - b2xyxy[..., 1])\n",
    "        size_A_union_B = area_A + area_B - intersection_area\n",
    "\n",
    "        # Calculate |C| - |A union B|\n",
    "        size_C = (C_xmax - C_xmin)*(C_ymax - C_ymin)\n",
    "        size_C_excluding_A_and_B = size_C - size_A_union_B\n",
    "\n",
    "        # Calculate (|C| - |A union B|)/|C|\n",
    "        ratio = size_C_excluding_A_and_B/size_C\n",
    "\n",
    "        # Calculate IoU - (|C| - |A union B|)/|C|\n",
    "        GIoU = iou - ratio\n",
    "\n",
    "        GIoU_loss = 1 - GIoU\n",
    "        return GIoU_loss\n",
    "\n",
    "    def iou_width_height(self, boxes1, boxes2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            boxes1 (tensor): width and height of the first bounding boxes\n",
    "            boxes2 (tensor): width and height of the second bounding boxes\n",
    "        Returns:\n",
    "            tensor: Intersection over union of the corresponding boxes\n",
    "        \"\"\"\n",
    "        intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
    "            boxes1[..., 1], boxes2[..., 1]\n",
    "        )\n",
    "        union = (\n",
    "            boxes1[..., 0] * boxes1[..., 1] +\n",
    "            boxes2[..., 0] * boxes2[..., 1] - intersection\n",
    "        )\n",
    "        return intersection / union\n",
    "\n",
    "    def bboxes_to_cellvectors(self, bboxes, anchors, S, ignore_iou_thresh):\n",
    "        \"\"\"\n",
    "        This function converts a list of bboxes to the targets for each scale\n",
    "        input bboxes: a list of bboxes in the form [x, y, width, height]\n",
    "        returns: a list of size 3 with a target vector for each scale.\n",
    "        \"\"\"\n",
    "        num_anchors = anchors.shape[0]\n",
    "        num_anchors_per_scale = num_anchors // 3\n",
    "\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((num_anchors // 3, S, S, 5)) for S in S]\n",
    "\n",
    "        for box in bboxes:\n",
    "\n",
    "            # get a list of the IOU for all anchor boxes for this bounding box\n",
    "            iou_anchors = UTILS.iou_width_height(\n",
    "                torch.tensor(box[2:4]), anchors)\n",
    "            # sort the list of IOU's to get the largest values first\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "\n",
    "            x, y, width, height = box\n",
    "            # each scale should have one anchor (we want to pick one anchor per scale for each bounding box)\n",
    "            has_anchor = [False] * 3\n",
    "\n",
    "            for anchor_idx in anchor_indices:  # find an anchor that fits\n",
    "                # determine which scale we're working with\n",
    "                scale_idx = anchor_idx // num_anchors_per_scale\n",
    "                # determines the anchor number on that scale\n",
    "                anchor_on_scale = anchor_idx % num_anchors_per_scale\n",
    "                s = S[scale_idx]  # get the grid size\n",
    "                # determine the cell within the grid cell\n",
    "                i, j = int(s * y), int(s * x)\n",
    "\n",
    "                # check the objectness value for this anchor to see if its already taken by by some other bbox\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale,\n",
    "                                                  i, j, 4] == 1\n",
    "\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "\n",
    "                    # set objectness prediction target to 1 (as there is an object here)\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 4] = 1\n",
    "                    # determine the x and y coordinates relative to the cell, both between [0,1]\n",
    "                    x_cell, y_cell = s * x - j, s * y - i\n",
    "\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width, height]\n",
    "                    )\n",
    "\n",
    "                    targets[scale_idx][anchor_on_scale,\n",
    "                                       i, j, 0:4] = box_coordinates\n",
    "\n",
    "                    # notify that the bbox has an anchor for this scale\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                # if there is already an anchor for this bbox (so with a better iou)\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > ignore_iou_thresh:\n",
    "                    # set objectness of this anchor to -1 to signal to ignore prediction\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 4] = -1\n",
    "\n",
    "        return targets\n",
    "\n",
    "    \"Unit test needed\"\n",
    "\n",
    "    def cellvector_to_bboxes(self, cellvector, anchors):\n",
    "        \"\"\"\n",
    "        This function converts the cellvector of one scale back to a list of bounding boxes\n",
    "        input cellvectors: cellvector of one scale (anchors per scale, S, S, 6))\n",
    "        input anchors: a list of anchors for this scale only\n",
    "        returns: a list of bounding boxes formatted as (x, y, w, h, obj_score)\n",
    "        \"\"\"\n",
    "        cellvector = cellvector.to(\"cpu\")\n",
    "\n",
    "        S = cellvector.shape[2]     # get S from the size of the cellvector\n",
    "\n",
    "        # the following code adds the index of the cell, both i and j, to the x and y positioin of the box relative to the cell. This results in the x and y relative to the image after dividing by the size of the grid\n",
    "        cell_indices_x = (\n",
    "            torch.arange(S)\n",
    "            .repeat(3, S, 1)\n",
    "        )\n",
    "\n",
    "        cell_indices_y = cell_indices_x.permute(0, 2, 1)\n",
    "\n",
    "        scale = 1/S\n",
    "        # x = (cx + i) / grid_scale\n",
    "        x = scale * (cellvector[..., 0] + cell_indices_x)\n",
    "        # y = (cy + j) / grid_scale\n",
    "        y = scale * (cellvector[..., 1] + cell_indices_y)\n",
    "        w = cellvector[..., 2]\n",
    "        h = cellvector[..., 3]\n",
    "        obj_pred = cellvector[..., 4]\n",
    "\n",
    "        bboxes = torch.stack((x, y, w, h, obj_pred), dim=-\n",
    "                             1).reshape(len(anchors) * S * S, 5)\n",
    "\n",
    "        return bboxes.tolist()\n",
    "\n",
    "    \"Unit test needed\"\n",
    "\n",
    "    def cellvectors_to_bboxes(self, cellvectors, anchors):\n",
    "        \"\"\"\n",
    "        This function converts the cellvectors of all scales back to a list of bounding boxes\n",
    "        input cellvectors: list with shape (#_of_scales,) of cellvectors per scale with shape (anchors per scale, S, S, 6))\n",
    "        input anchors: The complete list of list of anchors\n",
    "        returns: a list of bounding boxes formatted as (x, y, w, h, obj_score)\n",
    "        \"\"\"\n",
    "        assert len(cellvectors) == len(anchors)\n",
    "\n",
    "        batch_size = cellvectors[0].shape[0]\n",
    "\n",
    "        bboxes = []\n",
    "        for i in range(batch_size):\n",
    "            bboxes_batch = []\n",
    "            for scale_idx, cellvector in enumerate(cellvectors):\n",
    "                bboxes_batch.extend(self.cellvector_to_bboxes(\n",
    "                    cellvector[i], anchors[scale_idx]))\n",
    "            bboxes.append(bboxes_batch)\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    def non_max_suppression(self, bboxes, iou_threshold=CONFIG.NMS_IOU_THRESH, threshold=CONFIG.CONF_THRESHOLD, box_format=\"corners\"):\n",
    "        \"\"\"\n",
    "        Video explanation of this function:\n",
    "        https://youtu.be/YDkjWEN8jNA\n",
    "        Does Non Max Suppression given bboxes\n",
    "        Parameters:\n",
    "            bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "            specified as [x1, y1, x2, y2, obj_socre]\n",
    "            iou_threshold (float): threshold where predicted bboxes is correct\n",
    "            threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "            box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        Returns:\n",
    "            list: bboxes after performing NMS given a specific IoU threshold\n",
    "        \"\"\"\n",
    "        assert type(bboxes) == list\n",
    "\n",
    "        bboxes = [box for box in bboxes if box[4] > threshold]\n",
    "\n",
    "        if len(bboxes) > 0:\n",
    "            boxes = box_convert(torch.tensor([box[:4] for box in bboxes]).to(\n",
    "                CONFIG.DEVICE), 'xywh', 'xyxy')\n",
    "            scores = torch.tensor([box[4] for box in bboxes]).to(CONFIG.DEVICE)\n",
    "\n",
    "            return [bboxes[i] for i in nms(boxes, scores, iou_threshold).tolist()]\n",
    "        else:\n",
    "            return bboxes\n",
    "\n",
    "    def plot_image(self, image, boxes, yolo=True):\n",
    "        \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "\n",
    "        im = np.array(image)\n",
    "        height, width, _ = im.shape\n",
    "\n",
    "        # Create figure and axes\n",
    "        fig, ax = plt.subplots(1)\n",
    "        # Display the image\n",
    "        ax.imshow(im)\n",
    "\n",
    "        # box[0] is x midpoint, box[2] is width\n",
    "        # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        for box in boxes:\n",
    "            assert len(\n",
    "                box) == 5, \"box should contain x, y, width, height, obj_confidence, \"\n",
    "            box = box[:4]\n",
    "            if yolo:\n",
    "                upper_left_x = box[0] - box[2] / 2\n",
    "                upper_left_y = box[1] - box[3] / 2\n",
    "                rect = patches.Rectangle(\n",
    "                    (upper_left_x * width, upper_left_y * height),\n",
    "                    box[2] * width,\n",
    "                    box[3] * height,\n",
    "                    linewidth=2,\n",
    "                    edgecolor=\"blue\",\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "                # Add the patch to the Axes\n",
    "                ax.add_patch(rect)\n",
    "            else:  # coco/wider_face\n",
    "                upper_left_x = box[0]\n",
    "                upper_left_y = box[1]\n",
    "                rect = patches.Rectangle(\n",
    "                    (upper_left_x, upper_left_y),\n",
    "                    box[2],\n",
    "                    box[3],\n",
    "                    linewidth=2,\n",
    "                    edgecolor=\"blue\",\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "                # Add the patch to the Axes\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def get_evaluation_bboxes3(self,\n",
    "                               loader,\n",
    "                               model,\n",
    "                               iou_threshold=CONFIG.NMS_IOU_THRESH,\n",
    "                               anchors=CONFIG.ANCHORS,\n",
    "                               threshold=CONFIG.CONF_THRESHOLD,\n",
    "                               box_format=\"midpoint\",\n",
    "                               device=\"cuda\",\n",
    "                               ):\n",
    "\n",
    "        # Make sure model is in eval before get bboxes\n",
    "        model.eval()\n",
    "\n",
    "        all_pred_boxes = []\n",
    "        all_true_boxes = []\n",
    "\n",
    "        # go over all batches in the loader\n",
    "        for (x, y) in tqdm(loader):\n",
    "\n",
    "            # forward the entire batch trough the model to get a list of predictions for each batch\n",
    "            # shape of predictions = (#_of_scales = 3,) with tensors of the form (batch_size, #_of_anchors, S, S, 5)\n",
    "            x = x.to(CONFIG.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                predictions = model(x)\n",
    "\n",
    "            batch_size = y[0].shape[0]\n",
    "\n",
    "            bboxes_true_batch = UTILS.cellvectors_to_bboxes(y, anchors)\n",
    "\n",
    "            for bboxes_true in bboxes_true_batch:\n",
    "                bboxes_true = UTILS.non_max_suppression(bboxes_true)\n",
    "                all_true_boxes.append(bboxes_true)\n",
    "\n",
    "            bboxes_pred_batch = UTILS.cellvectors_to_bboxes(\n",
    "                predictions, anchors)\n",
    "\n",
    "            for bboxes_pred in bboxes_pred_batch:\n",
    "                bboxes_pred = UTILS.non_max_suppression(bboxes_pred)\n",
    "                all_pred_boxes.append(bboxes_pred)\n",
    "\n",
    "        return all_pred_boxes, all_true_boxes\n",
    "\n",
    "    def plot_examples(self,\n",
    "                      loader,\n",
    "                      model,\n",
    "                      number_of_examples=10,\n",
    "                      iou_threshold=CONFIG.NMS_IOU_THRESH,\n",
    "                      anchors=CONFIG.ANCHORS,\n",
    "                      threshold=CONFIG.CONF_THRESHOLD,\n",
    "                      box_format=\"midpoint\",\n",
    "                      device=\"cuda\",\n",
    "                      ):\n",
    "\n",
    "        assert loader.batch_size == 1\n",
    "\n",
    "        # Make sure model is in eval before get bboxes\n",
    "        model.eval()\n",
    "        train_idx = 0\n",
    "        all_pred_boxes = []\n",
    "        all_true_boxes = []\n",
    "        for batch_idx, (x, labels, true_bboxes) in enumerate(loader):\n",
    "\n",
    "            print(true_bboxes.shape)\n",
    "\n",
    "            image = x.squeeze().permute(1, 2, 0)\n",
    "            x = x.to(CONFIG.DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                predictions = model(x)\n",
    "\n",
    "            bboxes = []\n",
    "            for scale_idx in range(3):\n",
    "                S = predictions[scale_idx].shape[2]\n",
    "                anchor = torch.tensor(\n",
    "                    [*anchors[scale_idx]]).to(CONFIG.DEVICE) * S\n",
    "                boxes_scale_i = self.cells_to_bboxes(\n",
    "                    predictions[scale_idx], anchor, S=S, is_preds=True\n",
    "                )\n",
    "                for idx, (box) in enumerate(boxes_scale_i):\n",
    "                    bboxes += box\n",
    "\n",
    "            nms_boxes = self.non_max_suppression(\n",
    "                bboxes,\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            UTILS.plot_boxes(nms_boxes[:50], true_bboxes.squeeze(0), image)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    def get_mean_std(self, loader):\n",
    "        # var[X] = E[X**2] - E[X]**2\n",
    "        channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "        for data, _ in tqdm(loader):\n",
    "            channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "            channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "            num_batches += 1\n",
    "\n",
    "        mean = channels_sum / num_batches\n",
    "        std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def save_checkpoint(self, model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "        print(\"=> Saving checkpoint\")\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_file, model, optimizer, lr):\n",
    "        print(\"=> Loading checkpoint\")\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=CONFIG.DEVICE)\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "        # If we don't do this then it will just have learning rate of old checkpoint\n",
    "        # and it will lead to many hours of debugging \\:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def get_loaders(self):\n",
    "        trainSplit, valSplit, testSplit = CONFIG.dataSplit\n",
    "        total = CONFIG.numOfIm\n",
    "\n",
    "        trainStart = 0\n",
    "        trainEnd = int(total*trainSplit) - 1\n",
    "\n",
    "        valStart = trainEnd+1\n",
    "        valEnd = int(valStart+(total*valSplit))-1\n",
    "\n",
    "        testStart = valEnd+1\n",
    "        testEnd = int(testStart+(total*testSplit))\n",
    "\n",
    "        train_dataset = WIDER_Dataset(\n",
    "            CONFIG.INDEX_PATH_TRAIN,\n",
    "            CONFIG.IMAGE_STORE_PATH_TRAIN,\n",
    "            transform=CONFIG.test_transforms\n",
    "        )\n",
    "        train_dataset.setup(trainStart, trainEnd)\n",
    "\n",
    "        test_dataset = WIDER_Dataset(\n",
    "            CONFIG.INDEX_PATH_VAL,\n",
    "            CONFIG.IMAGE_STORE_PATH_VAL,\n",
    "            transform=CONFIG.test_transforms\n",
    "        )\n",
    "        test_dataset.setup(valStart, valEnd)\n",
    "\n",
    "        train_dataset_validation = WIDER_Dataset(\n",
    "            CONFIG.INDEX_PATH_TRAIN,\n",
    "            CONFIG.IMAGE_STORE_PATH_TRAIN,\n",
    "            transform=CONFIG.test_transforms,\n",
    "        )\n",
    "        train_dataset_validation.setup(1, 3)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=CONFIG.BATCH_SIZE,\n",
    "            num_workers=CONFIG.NUM_WORKERS,\n",
    "            pin_memory=CONFIG.PIN_MEMORY,\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=CONFIG.BATCH_SIZE,\n",
    "            num_workers=CONFIG.NUM_WORKERS,\n",
    "            pin_memory=CONFIG.PIN_MEMORY,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        train_loader_validation = DataLoader(\n",
    "            dataset=train_dataset_validation,\n",
    "            batch_size=2,\n",
    "            num_workers=0,\n",
    "            pin_memory=CONFIG.PIN_MEMORY,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        return train_loader, test_loader, train_loader_validation\n",
    "\n",
    "    def plot_couple_examples(self, model, loader, thresh, iou_thresh, anchors):\n",
    "        model.eval()\n",
    "        x, y = next(iter(loader))\n",
    "        x = x.to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            bboxes = [[] for _ in range(x.shape[0])]\n",
    "            for i in range(3):\n",
    "                batch_size, A, S, _, _ = out[i].shape\n",
    "                anchor = anchors[i]\n",
    "                boxes_scale_i = cells_to_bboxes(\n",
    "                    out[i], anchor, S=S, is_preds=True\n",
    "                )\n",
    "                for idx, (box) in enumerate(boxes_scale_i):\n",
    "                    bboxes[idx] += box\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
    "            )\n",
    "            self.plot_image(x[i].permute(1, 2, 0).detach().cpu(), nms_boxes)\n",
    "\n",
    "    def evaluate_model(self, test_loader, model):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        pred_boxes, true_boxes = UTILS.get_evaluation_bboxes3(\n",
    "            test_loader,\n",
    "            model\n",
    "        )\n",
    "\n",
    "        mAP = UTILS.mean_average_precision(\n",
    "            true_boxes=true_boxes, pred_boxes=pred_boxes)\n",
    "\n",
    "        # class_accuracy, no_obj_accuracy, obj_accuracy = UTILS.check_class_accuracy(model, test_loader, threshold=CONFIG.CONF_THRESHOLD)\n",
    "\n",
    "        return mAP  # , class_accuracy, no_obj_accuracy, obj_accuracy\n",
    "\n",
    "    def seed_everything(self, seed=42):\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "UTILS = Utils()\n",
    "\n",
    "class test_DIoU(unittest.TestCase):\n",
    "\n",
    "    def test_side_by_side(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        target = [[0.75, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        # The distance will be 0.5 --> rho^2 = 0.5^2 = 0.25\n",
    "        # c^2 will be 0.5^2 + (2*0.5)^2 = 1.25\n",
    "        # So R_DIoU = 0.25/1.25 = 0.2\n",
    "        R_DIoU = np.round(UTILS.DIoU_penalty(predictions, target), 5).squeeze()\n",
    "        self.assertEqual(R_DIoU, 0.2)\n",
    "\n",
    "        # The overlap is 0 so IOU=0, so loss should be 1-0+0.2 = 1.2\n",
    "        loss = np.round(UTILS.distance_intersection_over_union_loss(\n",
    "            predictions, target), 5)\n",
    "        self.assertEqual(loss, 1.2)\n",
    "\n",
    "    def test_overlap(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        target = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        # The distance will be 0 --> rho^2 = 0\n",
    "        # c^2 will be 0.5^2 + 0.5^2 = 0.5\n",
    "        # So R_DIoU = 0/0.5=0\n",
    "        R_DIoU = np.round(UTILS.DIoU_penalty(predictions, target), 5).squeeze()\n",
    "        self.assertEqual(R_DIoU, 0)\n",
    "\n",
    "        # The overlap is 0 so IOU=0, so loss should be 1-1+0 = 0\n",
    "        loss = np.round(UTILS.distance_intersection_over_union_loss(\n",
    "            predictions, target), 5).squeeze()\n",
    "        self.assertEqual(loss, 0)\n",
    "\n",
    "\n",
    "class test_CIoU(unittest.TestCase):\n",
    "\n",
    "    def test_side_by_side(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh, aspect ratio 1\n",
    "\n",
    "        target = [[0.75, 0.25, 0.5, 0.5]]  # cxcywh, aspect ratio 1\n",
    "\n",
    "        R_CIoU = np.round(UTILS.CIoU_penalty(predictions, target), 5)\n",
    "        self.assertEqual(R_CIoU, 0.2)\n",
    "\n",
    "        # The overlap is 0 so IOU=0, so loss should be 1-0+0.2 = 1.2\n",
    "        loss = np.round(UTILS.distance_intersection_over_union_loss(\n",
    "            predictions, target), 5).squeeze()\n",
    "        self.assertEqual(loss, 1.2)\n",
    "\n",
    "    def test_overlap(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        target = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        # Same aspect ratio, so v=0 and alpha=0.\n",
    "        # The distance will be 0 --> rho^2 = 0\n",
    "        # c^2 will be 0.5^2 + 0.5^2 = 0.5\n",
    "        # So R_DIoU = 0/0.5=0\n",
    "        R_CIoU = UTILS.CIoU_penalty(predictions, target)\n",
    "        self.assertEqual(R_CIoU, 0)\n",
    "\n",
    "        # The overlap is 0 so IOU=0, so loss should be 1-1+0 = 0\n",
    "        loss = np.round(UTILS.distance_intersection_over_union_loss(\n",
    "            predictions, target), 5).squeeze()\n",
    "        self.assertEqual(loss, 0)\n",
    "\n",
    "\n",
    "class test_GIoU(unittest.TestCase):\n",
    "\n",
    "    def test_side_by_side(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "        target = [[0.75, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        # A union B is 0.5\n",
    "        # A intersection B = 0\n",
    "        # IOU = 0\n",
    "        # |C| = 0.5\n",
    "        # GIoU = 0 - (0/0.5) = 0\n",
    "        # GIoU loss = 1 - 0 = 0\n",
    "        loss = UTILS.generalised_intersection_over_union_loss(\n",
    "            predictions, target).squeeze().item()\n",
    "        self.assertEqual(loss, 1)\n",
    "\n",
    "    def test_overlap(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "        target = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        # A union B is 0.25\n",
    "        # A intersection B = 0.25\n",
    "        # IOU = 1\n",
    "        # |C| = 0.25\n",
    "        # GIoU = 1 - (0/0.25) = 1\n",
    "        # GIoU loss = 1 - 1 = 0\n",
    "        loss = UTILS.generalised_intersection_over_union_loss(\n",
    "            predictions, target).squeeze().item()\n",
    "        self.assertEqual(loss, 0)\n",
    "\n",
    "    def test_one_corner_touching(self):\n",
    "        predictions = [[0.25, 0.25, 0.5, 0.5]]  # cxcywh\n",
    "        target = [[0.75, 0.75, 0.5, 0.5]]  # cxcywh\n",
    "\n",
    "        # A union B is 0.5\n",
    "        # A intersection B = 0\n",
    "        # IOU = 0\n",
    "        # |C| = 1\n",
    "        # |C excluding A and B| / |C| = 0.5 / 1 = 0.5\n",
    "        # GIoU = 0 - 0.5 = -0.5\n",
    "        # GIoU loss = 1 - (-0.5) = 1.5\n",
    "        loss = UTILS.generalised_intersection_over_union_loss(\n",
    "            predictions, target).squeeze().item()\n",
    "        self.assertEqual(loss, 1.5)\n",
    "\n",
    "class test_bboxes_to_cellvectors(unittest.TestCase):\n",
    "\n",
    "    def itterateEqual(self, expectedOutcome, cellVectors):\n",
    "        # print(expectedOutcome[0].shape)\n",
    "        for l in range(3):\n",
    "            for bb in range(self.num_anchors//3):\n",
    "                for s1 in range(self.S[l]):\n",
    "                    for s2 in range(self.S[l]):\n",
    "                        for j in range(5):\n",
    "                            # Should result in a faulty test? It might return a 0 vector because non of the bb have a good enough matching iou?\n",
    "                            # See how the bboxes are defined/calculated (in Widerfaces_dataset).\n",
    "                            self.assertEqual(\n",
    "                                expectedOutcome[l][bb][s1][s2][j], cellVectors[l][bb][s1][s2][j])\n",
    "\n",
    "    def setUp(self):\n",
    "        self.anchors = torch.tensor(\n",
    "            CONFIG.ANCHORS[0] + CONFIG.ANCHORS[1] + CONFIG.ANCHORS[2])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.S = CONFIG.S\n",
    "\n",
    "    def test_single_box(self):\n",
    "        # test a single box in the middle of a image:\n",
    "        x, y, w, h = 0.5, 0.5, 0.1, 0.1\n",
    "        bboxes = [[x, y, w, h]]  # list of bb that have to be converted\n",
    "\n",
    "        cellVectors = UTILS.bboxes_to_cellvectors(\n",
    "            bboxes, self.anchors, CONFIG.S, 0.5)  # result from function\n",
    "\n",
    "        # expected result:\n",
    "        expectedOutcome = [torch.zeros(\n",
    "            (self.num_anchors // 3, S, S, 5)) for S in self.S]\n",
    "        # note that the cellvectors have their width and height relative to the image size, not relative to the bboxes.\n",
    "        expectedOutcome[0][0][6][6] = torch.tensor(\n",
    "            [0.5, 0.5, 0.1, 0.1, 1])  # add bbox to the first layer\n",
    "        # add ignore flag to the second layer (cause there are two well matchin bboxes on this layer)\n",
    "        expectedOutcome[1][0][13][13] = torch.tensor([0, 0, 0, 0, -1])\n",
    "        # add bbox to the second layer (this is the one witht he highest IOU)\n",
    "        expectedOutcome[1][1][13][13] = torch.tensor([0, 0, 0.1, 0.1, 1])\n",
    "        # add bbox to the third layer (this one has a low IOU of 0.48 but should still be added as it is the highest)\n",
    "        expectedOutcome[2][2][26][26] = torch.tensor([0, 0, 0.1, 0.1, 1])\n",
    "\n",
    "        self.itterateEqual(expectedOutcome, cellVectors)\n",
    "\n",
    "    def test_overlapping_boxes(self):  # tests what happens when boxes overlap\n",
    "        ### overlap, but equal shape ###\n",
    "        x, y, w, h = 0.5, 0.5, 0.1, 0.1\n",
    "        # list of bb that have to be converted\n",
    "        bboxes = [[x, y, w, h], [x+0.01, y+0.01, w, h]]\n",
    "\n",
    "        cellVectors = UTILS.bboxes_to_cellvectors(\n",
    "            bboxes, self.anchors, CONFIG.S, 0.5)  # result from function\n",
    "\n",
    "        # expected result:\n",
    "        expectedOutcome = [torch.zeros(\n",
    "            (self.num_anchors // 3, S, S, 5)) for S in self.S]\n",
    "        # note that the cellvectors have their width and height relative to the image size, not relative to the bboxes.\n",
    "        # make changes for the first bbox\n",
    "        expectedOutcome[0][0][6][6] = torch.tensor(\n",
    "            [0.5, 0.5, 0.1, 0.1, 1])  # add bbox to the first layer\n",
    "        # add ignore flag to the second layer (cause there are two well matchin bboxes on this layer)\n",
    "        expectedOutcome[1][0][13][13] = torch.tensor([0, 0, 0, 0, -1])\n",
    "        # add bbox to the second layer (this is the one witht he highest IOU)\n",
    "        expectedOutcome[1][1][13][13] = torch.tensor([0, 0, 0.1, 0.1, 1])\n",
    "        # add bbox to the third layer (this one has a low IOU of 0.48 but should still be added as it is the highest)\n",
    "        expectedOutcome[2][2][26][26] = torch.tensor([0, 0, 0.1, 0.1, 1])\n",
    "        # make changes for the second bbox:\n",
    "        expectedOutcome[0][1][6][6] = torch.tensor([0.63, 0.63, 0.1, 0.1, 1])\n",
    "        # the second box should override the 'ignore' flag of the first box\n",
    "        expectedOutcome[1][0][13][13] = torch.tensor([0.26, 0.26, 0.1, 0.1, 1])\n",
    "        expectedOutcome[2][1][26][26] = torch.tensor([0.52, 0.52, 0.1, 0.1, 1])\n",
    "\n",
    "        self.itterateEqual(expectedOutcome, cellVectors)\n",
    "\n",
    "        ### overlap non-equal shape ###\n",
    "        x, y, w, h = 0.5, 0.5, 0.1, 0.1\n",
    "        # list of bb that have to be converted\n",
    "        bboxes = [[x, y, w, h], [x+0.01, y+0.01, w+0.01, h+0.01]]\n",
    "\n",
    "        cellVectors = UTILS.bboxes_to_cellvectors(\n",
    "            bboxes, self.anchors, CONFIG.S, 0.5)  # result from function\n",
    "\n",
    "        # expected result:\n",
    "        expectedOutcome = [torch.zeros(\n",
    "            (self.num_anchors // 3, S, S, 5)) for S in self.S]\n",
    "        # note that the cellvectors have their width and height relative to the image size, not relative to the bboxes.\n",
    "        # make changes for the first bbox\n",
    "        expectedOutcome[0][0][6][6] = torch.tensor(\n",
    "            [0.5, 0.5, 0.1, 0.1, 1])  # add bbox to the first layer\n",
    "        # add ignore flag to the second layer (cause there are two well matchin bboxes on this layer)\n",
    "        expectedOutcome[1][0][13][13] = torch.tensor([0, 0, 0, 0, -1])\n",
    "        # add bbox to the second layer (this is the one witht he highest IOU)\n",
    "        expectedOutcome[1][1][13][13] = torch.tensor([0, 0, 0.1, 0.1, 1])\n",
    "        # add bbox to the third layer (this one has a low IOU of 0.48 but should still be added as it is the highest)\n",
    "        expectedOutcome[2][2][26][26] = torch.tensor([0, 0, 0.1, 0.1, 1])\n",
    "        # make changes for the second bbox:\n",
    "        # This box should get priority for bb number 0 since the biger size gives it a higher iou, but it does not atm. Is this a problem?\n",
    "        expectedOutcome[0][1][6][6] = torch.tensor([0.63, 0.63, 0.11, 0.11, 1])\n",
    "        # the second box should override the 'ignore' flag of the first box\n",
    "        expectedOutcome[1][0][13][13] = torch.tensor(\n",
    "            [0.26, 0.26, 0.11, 0.11, 1])\n",
    "        expectedOutcome[2][1][26][26] = torch.tensor(\n",
    "            [0.52, 0.52, 0.11, 0.11, 1])\n",
    "\n",
    "        self.itterateEqual(expectedOutcome, cellVectors)\n",
    "\n",
    "    def test_bboxNearSides(self):\n",
    "        ### place the bbox near the sides so that it spills over ###\n",
    "        x, y, w, h = 0.95, 0.95, 0.1, 0.1\n",
    "        bboxes = [[x, y, w, h]]  # list of bb that have to be converted\n",
    "\n",
    "        cellVectors = UTILS.bboxes_to_cellvectors(\n",
    "            bboxes, self.anchors, CONFIG.S, 0.5)  # result from function\n",
    "\n",
    "        # expected result:\n",
    "        expectedOutcome = [torch.zeros(\n",
    "            (self.num_anchors // 3, S, S, 5)) for S in self.S]\n",
    "        # note that the cellvectors have their width and height relative to the image size, not relative to the bboxes.\n",
    "        # make changes for the first bbox\n",
    "        expectedOutcome[0][0][12][12] = torch.tensor(\n",
    "            [0.35, 0.35, 0.1, 0.1, 1])  # add bbox to the first layer\n",
    "        # add ignore flag to the second layer (cause there are two well matchin bboxes on this layer)\n",
    "        expectedOutcome[1][0][24][24] = torch.tensor([0, 0, 0, 0, -1])\n",
    "        # add bbox to the second layer (this is the one witht he highest IOU)\n",
    "        expectedOutcome[1][1][24][24] = torch.tensor([0.7, 0.7, 0.1, 0.1, 1])\n",
    "        # add bbox to the third layer (this one has a low IOU of 0.48 but should still be added as it is the highest)\n",
    "        expectedOutcome[2][2][49][49] = torch.tensor([0.4, 0.4, 0.1, 0.1, 1])\n",
    "\n",
    "        self.itterateEqual(expectedOutcome, cellVectors)\n",
    "\n",
    "class test_non_max_supressions(unittest.TestCase):\n",
    "\n",
    "    def test_SingleBbLowConfidence(self):\n",
    "        #[x1, y1, x2, y2, obj_socre]\n",
    "        bboxes = [[0.5, 0.5, 0.1, 0.1, 0.5]]\n",
    "        outcome = UTILS.non_max_suppression(bboxes)\n",
    "        self.assertEqual(outcome, [])\n",
    "\n",
    "    def test_SingleBbHighConfidence(self):\n",
    "        #[x1, y1, x2, y2, obj_socre]\n",
    "        bboxes = [[0.5, 0.5, 0.1, 0.1, 0.8]]\n",
    "        outcome = UTILS.non_max_suppression(bboxes)\n",
    "        self.assertEqual(outcome, bboxes)\n",
    "\n",
    "    def test_overlappingLowIou(self):\n",
    "        #[x1, y1, x2, y2, obj_socre]\n",
    "        bboxes = [[0.5, 0.5, 0.1, 0.1, 0.8], [0.58, 0.5, 0.1, 0.1, 0.8]]\n",
    "        outcome = UTILS.non_max_suppression(bboxes)\n",
    "\n",
    "        # make sure both lists are in the same order before running test\n",
    "        bboxes.sort()\n",
    "        outcome.sort()\n",
    "\n",
    "        self.assertEqual(outcome, bboxes)\n",
    "\n",
    "    def test_overlappingHighIou(self):\n",
    "        #[x1, y1, x2, y2, obj_socre]\n",
    "        bboxes = [[0.5, 0.5, 0.1, 0.1, 0.8], [0.56, 0.5, 0.1, 0.1, 0.9]]\n",
    "        outcome = UTILS.non_max_suppression(bboxes)\n",
    "\n",
    "        self.assertEqual(outcome, [[0.56, 0.5, 0.1, 0.1, 0.9]])\n",
    "\n",
    "    def test_overlappingMixedIou(self):\n",
    "        #[x1, y1, x2, y2, obj_socre]\n",
    "        bboxes = [[0.5, 0.5, 0.1, 0.1, 0.8], [\n",
    "            0.64, 0.5, 0.1, 0.1, 0.8], [0.56, 0.5, 0.1, 0.1, 0.9]]\n",
    "        outcome = UTILS.non_max_suppression(bboxes)\n",
    "\n",
    "        # make sure both lists are in the same order before running test\n",
    "        expectedOutcome = [[0.64, 0.5, 0.1, 0.1, 0.8],\n",
    "                           [0.56, 0.5, 0.1, 0.1, 0.9]]\n",
    "        expectedOutcome.sort()\n",
    "        outcome.sort()\n",
    "\n",
    "        self.assertEqual(outcome, expectedOutcome)\n",
    "\n",
    "class TestMAP(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.margin = 1e-5\n",
    "\n",
    "    def test_false_positive_simple(self):\n",
    "        predictions = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 1]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target = [\n",
    "            [\n",
    "                [0.9, 0.9, 0.01, 0.01]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions, true_boxes=target, iou_threshold=0.5)\n",
    "        manual_score = 0\n",
    "\n",
    "        self.assertTrue(abs(score - manual_score) < self.margin)\n",
    "\n",
    "    def test_true_positive_simple(self):\n",
    "        predictions = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 1]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions, true_boxes=target, iou_threshold=0.5)\n",
    "        manual_score = 1\n",
    "\n",
    "        self.assertTrue(abs(score - manual_score) < self.margin)\n",
    "\n",
    "    def test_single_sample_complex(self):\n",
    "        predictions = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 1],\n",
    "                [0.9, 0.9, 0.01, 0.01, 0],\n",
    "                [0.1, 0.1, 0.1, 0.1, 0.5]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target = [\n",
    "            [\n",
    "                [0.1, 0.1, 0.01, 0.01, 1],\n",
    "                [0.5, 0.5, 0.2, 0.2, 1],\n",
    "                [0.9, 0.9, 0.01, 0.0099, 1]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions, true_boxes=target, iou_threshold=0.5)\n",
    "\n",
    "        manual_score = (1 + 0.75) / 2 * 1/3 + (0.75 + 0.6667) / 2 * 1/3\n",
    "\n",
    "        self.assertTrue(abs(score - manual_score) < self.margin)\n",
    "\n",
    "    def test_false_positive_multiple_simple(self):\n",
    "        predictions = [\n",
    "            [\n",
    "\n",
    "            ],\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 1]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2]\n",
    "            ],\n",
    "            [\n",
    "\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions, true_boxes=target, iou_threshold=0.5)\n",
    "\n",
    "        manual_score = 0\n",
    "\n",
    "        self.assertTrue(abs(score - manual_score) < self.margin)\n",
    "\n",
    "    def test_multiple_complex(self):\n",
    "        predictions = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 0.9],\n",
    "                [0.5, 0.5, 0.2, 0.2, 0.8],\n",
    "                [0.9, 0.9, 0.001, 0.001, 0.7]\n",
    "            ],\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 1]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2]\n",
    "            ],\n",
    "            [\n",
    "                [0.9, 0.9, 0.01, 0.01]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions, true_boxes=target, iou_threshold=0.5)\n",
    "\n",
    "        manual_score = (0 + 0.5)/2 * 0.5\n",
    "\n",
    "        self.assertTrue(abs(score - manual_score) < self.margin)\n",
    "\n",
    "\n",
    "    def test_tiny_filter(self):\n",
    "        predictions_1 = [\n",
    "            [\n",
    "                [0.2, 0.8, 0.25, 0.25, 1],\n",
    "                [0.5, 0.5, 0.2, 0.2, 0.95]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        predictions_2 = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2, 0.95]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target_1 = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        target_2 = [\n",
    "            [\n",
    "                [0.5, 0.5, 0.2, 0.2],\n",
    "                [0.2, 0.8, 0.25, 0.25]\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        score_1 = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions_2, true_boxes=target_1, iou_threshold=0.5)\n",
    "        score_2 = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions_1, true_boxes=target_1, iou_threshold=0.5)\n",
    "        score_3 = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions_1, true_boxes=target_1, iou_threshold=0.5, tiny=True, filter_area=0.06)\n",
    "        score_4 = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions_1, true_boxes=target_1, iou_threshold=0.5, tiny=True, filter_area=0.07)\n",
    "        score_5 = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions_2, true_boxes=target_2, iou_threshold=0.5)\n",
    "        score_6 = UTILS.mean_average_precision(\n",
    "            pred_boxes=predictions_2, true_boxes=target_2, iou_threshold=0.5, tiny=True, filter_area=0.06)\n",
    "        score_7 = UTILS.mean_average_precision(\n",
    "        pred_boxes=predictions_2, true_boxes=target_2, iou_threshold=0.5, tiny=True, filter_area=0.07)\n",
    "\n",
    "        self.assertNotAlmostEqual(score_1, score_2)\n",
    "        self.assertAlmostEqual(score_1, score_3)\n",
    "        self.assertAlmostEqual(score_2, score_4)\n",
    "        self.assertNotAlmostEqual(score_3, score_4)\n",
    "        self.assertNotAlmostEqual(score_5,score_1)\n",
    "        self.assertAlmostEqual(score_6, score_1)\n",
    "        self.assertNotAlmostEqual(score_7, score_6)\n",
    "        self.assertAlmostEqual(score_7, score_5)\n",
    "\n",
    "\n",
    "\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-i7tSfjThwV"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBN8dgJ9Tjvd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a Pytorch dataset to load the WIDER face dataset\n",
    "\"\"\"\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "def clamp(n, minn, maxn):\n",
    "    return max(min(maxn, n), minn)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function returns a dictionary containing all training samples, for each sample containing the image location (\"file_name\") and defention of the bounding boxes (\"bounding_boxes\")\n",
    "returns: the above discribed dict\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def read_data_index(index_path, skip_big_bb=False):\n",
    "    training_set = []\n",
    "\n",
    "    f = open(index_path, \"r\")\n",
    "    while True:\n",
    "        file_name = f.readline().strip()\n",
    "\n",
    "        if not file_name:\n",
    "            break\n",
    "\n",
    "        # incl = True #A boolean that tracks whether an image includes ANY large faces, if this bool is set to false the whole image will be skipped\n",
    "            # (in the case that skip_big_bb = True)\n",
    "        boxes = []\n",
    "        isSmall = []\n",
    "        box_count = int(f.readline())\n",
    "        if box_count >= 1:\n",
    "            for i in range(box_count):\n",
    "                box = f.readline().strip().split(' ')\n",
    "\n",
    "                treshold = CONFIG.BB_TRESHOLD\n",
    "                if int(box[2]) < treshold:\n",
    "                    box = tuple([int(x) for x in box])\n",
    "                    boxes.append(box)\n",
    "                elif not skip_big_bb:\n",
    "                    box = tuple([int(x) for x in box])\n",
    "                    boxes.append(box)\n",
    "                # else:\n",
    "                #    incl = False\n",
    "\n",
    "        else:\n",
    "            next(f)\n",
    "\n",
    "        instance = {\n",
    "            \"file_name\": file_name,\n",
    "            \"bounding_boxes\": boxes\n",
    "        }\n",
    "\n",
    "        training_set.append(instance)\n",
    "\n",
    "        # if incl:\n",
    "        #    instance = {\n",
    "        #        \"file_name\": file_name,\n",
    "        #        \"bounding_boxes\": boxes\n",
    "        #        }\n",
    "\n",
    "        #    training_set.append(instance)\n",
    "        # print(\"new instance: \" + str(instance))\n",
    "\n",
    "    return training_set\n",
    "\n",
    "\n",
    "class WIDER_Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        index_path,\n",
    "        image_store_path,\n",
    "        transform=None,\n",
    "        max_size=None,\n",
    "        skip_first=0,\n",
    "        anchors=CONFIG.ANCHORS,\n",
    "        image_size=CONFIG.IMAGE_SIZE,\n",
    "        S=CONFIG.S\n",
    "    ):\n",
    "        self.index_path = index_path\n",
    "        self.image_store_path = image_store_path\n",
    "        self.data_index = read_data_index(index_path, False)\n",
    "        self.max_size = max_size\n",
    "        self.skip_first = skip_first\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(\n",
    "            anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "        self.DEFAULT_FACE_CLASS = 1\n",
    "        self.beginIndex = 0\n",
    "        self.endIndex = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        # return (len(self.data_index) - self.skip_first if not self.max_size else self.max_size)\n",
    "        return (self.endIndex - self.beginIndex + 1)\n",
    "\n",
    "    def setup(self, begin, end):\n",
    "        # index of the first item that is part of this dataset.\n",
    "        self.beginIndex = begin\n",
    "        # index of the last item that is part of this dataset.\n",
    "        self.endIndex = end\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index + self.skip_first + self.beginIndex\n",
    "        index_item = self.data_index[index]\n",
    "        bboxes = index_item.get('bounding_boxes')\n",
    "        img_path = self.image_store_path + index_item.get('file_name')\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        image2 = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # [left, top, width, height, score]\n",
    "        bboxes = [[bbox[0], bbox[1], bbox[2], bbox[3]] if bbox[2] > 0 and bbox[3] > 0 else\n",
    "                  [bbox[0], bbox[1], bbox[2], 1] if bbox[2] > 0 else\n",
    "                  [bbox[0], bbox[1], 1, bbox[3]] if bbox[3] > 0 else\n",
    "                  [bbox[0], bbox[1], 1, 1] for bbox in bboxes]\n",
    "\n",
    "        # bboxes = [[bbox[0],bbox[1],bbox[2],bbox[3]] for bbox in bboxes if bbox[2] > 0 and bbox[3] > 0]\n",
    "        try:\n",
    "            if self.transform:\n",
    "                augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "                image = augmentations[\"image\"]\n",
    "                bboxes = augmentations[\"bboxes\"]\n",
    "        except:\n",
    "            for box in bboxes:\n",
    "                try:\n",
    "                    augmentations = self.transform(image=image2, bboxes=[box])\n",
    "                except Exception as error:\n",
    "                    print(img_path)\n",
    "                    print(box)\n",
    "                    print(error)\n",
    "\n",
    "        # convert from WIDER X (left) Y (top) width height (absolute) to YOLO X (centre) Y (centre) Width Height (with respect to the image)\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[2]\n",
    "\n",
    "        bboxes = [[max(0, min((bbox[0] + 0.5 * bbox[2] - 1) / image_width, 1)),\n",
    "                   max(0, min(\n",
    "                       (bbox[1] + 0.5 * bbox[3] - 1) / image_height, 1)),\n",
    "                   min(bbox[2] / image_width, 1),\n",
    "                   min(bbox[3] / image_height, 1)]\n",
    "                  for bbox in bboxes]\n",
    "\n",
    "        targets = UTILS.bboxes_to_cellvectors(\n",
    "            bboxes, self.anchors, self.S, self.ignore_iou_thresh)\n",
    "\n",
    "        return image, targets  # , torch.Tensor(bboxes)\n",
    "        # return image, bboxes\n",
    "\n",
    "# class WIDER_Dataset(Dataset):\n",
    "#    def __init__(\n",
    "#            self,\n",
    "#            index_path,\n",
    "#            image_store_path,\n",
    "#            transform=None,\n",
    "#            max_size=None,\n",
    "#            skip_first=0,\n",
    "#            anchors=CONFIG.ANCHORS,\n",
    "#            image_size=CONFIG.IMAGE_SIZE,\n",
    "#            S=CONFIG.S\n",
    "#    ):\n",
    "#        self.index_path = index_path\n",
    "#        self.image_store_path = image_store_path\n",
    "#        self.data_index = read_data_index(index_path)\n",
    "#        self.max_size = max_size\n",
    "#        self.skip_first = skip_first\n",
    "#        self.transform = transform\n",
    "#        self.S = S\n",
    "#        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "#        self.num_anchors = self.anchors.shape[0]\n",
    "#        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "#        self.ignore_iou_thresh = 0.5\n",
    "#        self.DEFAULT_FACE_CLASS = 1\n",
    "\n",
    "#    def __len__(self):\n",
    "#        return (len(self.data_index) - self.skip_first if not self.max_size else self.max_size)\n",
    "\n",
    "#    def __getitem__(self, index):\n",
    "\n",
    "#        index = index + self.skip_first\n",
    "\n",
    "#        index_item = self.data_index[index]\n",
    "#        bboxes = index_item.get('bounding_boxes')\n",
    "#        img_path = self.image_store_path + index_item.get('file_name')\n",
    "\n",
    "#        image = Image.open(img_path).convert(\"RGB\")\n",
    "#        image = np.array(image)\n",
    "#        image2 = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # [left, top, width, height, score]\n",
    "#        bboxes = [[bbox[0],bbox[1],bbox[2],bbox[3]] if bbox[2] > 0 and bbox[3] > 0 else\n",
    "#                  [bbox[0],bbox[1],bbox[2],1] if bbox[2] > 0 else\n",
    "#                  [bbox[0],bbox[1],1,bbox[3]] if bbox[3] > 0 else\n",
    "#                  [bbox[0],bbox[1],1,1] for bbox in bboxes]\n",
    "\n",
    "        # bboxes = [[bbox[0],bbox[1],bbox[2],bbox[3]] for bbox in bboxes if bbox[2] > 0 and bbox[3] > 0]\n",
    "#        try:\n",
    "#            if self.transform:\n",
    "#                augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "#                image = augmentations[\"image\"]\n",
    "#                bboxes = augmentations[\"bboxes\"]\n",
    "#        except:\n",
    "#            for box in bboxes:\n",
    "#                try:\n",
    "#                    augmentations = self.transform(image=image2, bboxes=[box])\n",
    "#                except Exception as error:\n",
    "#                    print(img_path)\n",
    "#                    print(box)\n",
    "#                    print(error)\n",
    "\n",
    "        # convert from WIDER X (left) Y (top) width height (absolute) to YOLO X (centre) Y (centre) Width Height (with respect to the image)\n",
    "#        image_width = image.shape[1]\n",
    "#        image_height = image.shape[2]\n",
    "\n",
    "#        bboxes = [[max(0,min((bbox[0] + 0.5 * bbox[2] - 1) / image_width,1)),\n",
    "#                   max(0,min((bbox[1] + 0.5 * bbox[3] - 1) / image_height,1)),\n",
    "#                   min(bbox[2] / image_width,1),\n",
    "#                   min(bbox[3] / image_height,1)]\n",
    "#                  for bbox in bboxes]\n",
    "\n",
    "#        targets = UTILS.bboxes_to_cellvectors(bboxes, self.anchors, self.S, self.ignore_iou_thresh)\n",
    "\n",
    "#        return image, targets#, torch.Tensor(bboxes)\n",
    "        # return image, bboxes\n",
    "\n",
    "\n",
    "def test():\n",
    "    anchors = CONFIG.ANCHORS\n",
    "\n",
    "    transform = CONFIG.test_transforms\n",
    "\n",
    "    dataset = WIDER_Dataset(\n",
    "        CONFIG.INDEX_PATH_ALL,\n",
    "        '../wider_face/',\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
    "    for x, y in loader:\n",
    "        print(np.shape(y[0]))\n",
    "\n",
    "        for vector in y[0][0].view(3*13*13, 5):\n",
    "            if vector[4] != 0:\n",
    "                print(vector)\n",
    "\n",
    "        bboxes_batch = UTILS.cellvectors_to_bboxes(y, anchors)\n",
    "\n",
    "        bboxes = UTILS.non_max_suppression(\n",
    "            bboxes_batch[0], iou_threshold=1, threshold=0.7)\n",
    "\n",
    "        # print(bboxes)\n",
    "\n",
    "        UTILS.plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), bboxes)\n",
    "\n",
    "        break\n",
    "\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = [[528, 256, 153, 169, 1],\n",
    "          [341, 309, 110, 131, 1],\n",
    "          [444, 220, 88, 118, 1],\n",
    "          [163, 145, 32, 45, 1],\n",
    "          [284, 155, 33, 36, 1],\n",
    "          [770, 122, 109, 136, 1],\n",
    "          [966, 77, 55, 64, 1],\n",
    "          [969, 198, 55, 70, 1],\n",
    "          [338, 144, 29, 45, 1],\n",
    "          [427, 151, 29, 39, 1],\n",
    "          [407, 139, 28, 41, 1],\n",
    "          [222, 170, 22, 34, 1],\n",
    "          [240, 167, 31, 33, 1],\n",
    "          [0, 138, 18, 30, 2],\n",
    "          [86, 146, 24, 35, 1],\n",
    "          [138, 172, 26, 27, 1],\n",
    "          [487, 155, 32, 35, 1],\n",
    "          [468, 153, 21, 34, 1],\n",
    "          [499, 117, 28, 32, 2],\n",
    "          [535, 101, 35, 47, 1],\n",
    "          [708, 69, 53, 78, 1],\n",
    "          [687, 69, 44, 59, 1],\n",
    "          [788, 69, 31, 30, 2],\n",
    "          [633, 35, 15, 21, 2],\n",
    "          [663, 31, 14, 18, 2],\n",
    "          [724, 22, 12, 13, 2],\n",
    "          [741, 14, 16, 15, 2],\n",
    "          # not visible and x bigger than image size (1024)\n",
    "          [1050, 142, 0, 50, 2],\n",
    "          [771, 0, 19, 15, 2],\n",
    "          [70, 195, 22, 30, 2],\n",
    "          [852, 52, 52, 68, 1],\n",
    "          [10, 388, 87, 71, 1]]\n",
    "\n",
    "img_path = \"../wider_face/train/54--Rescue/54_Rescue_rescuepeople_54_29.jpg\"\n",
    "image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "UTILS.plot_image(image, bboxes, yolo=False)\n",
    "\n",
    "bboxes = [[784, 148, 27, 33, 2],\n",
    "          [696, 202, 10, 13, 2],\n",
    "          [304, 88, 20, 29, 2],\n",
    "          [352, 183, 20, 25, 2],\n",
    "          [445, 134, 32, 46, 1],\n",
    "          [460, 224, 45, 33, 1],\n",
    "          [395, 97, 16, 25, 2],\n",
    "          # not visible and x bigger than image size (1024)\n",
    "          [1026, 474, 0, 23, 2],\n",
    "          [384, 97, 11, 15, 2],\n",
    "          [521, 114, 27, 40, 2],\n",
    "          [499, 137, 21, 22, 2],\n",
    "          [325, 86, 18, 25, 2],\n",
    "          [303, 173, 17, 22, 2],\n",
    "          [274, 74, 13, 13, 2],\n",
    "          [329, 72, 13, 13, 2],\n",
    "          [652, 198, 10, 11, 2],\n",
    "          [661, 210, 10, 9, 2],\n",
    "          [676, 206, 9, 13, 2],\n",
    "          [668, 201, 8, 10, 2],\n",
    "          [599, 177, 9, 12, 2],\n",
    "          [627, 173, 9, 11, 2],\n",
    "          [626, 137, 5, 7, 2],\n",
    "          [619, 164, 5, 6, 2],\n",
    "          [639, 212, 7, 9, 2],\n",
    "          [418, 108, 19, 25, 2],\n",
    "          [359, 94, 16, 17, 2],\n",
    "          [612, 188, 10, 11, 2],\n",
    "          [653, 178, 10, 11, 2],\n",
    "          [634, 220, 20, 17, 2],\n",
    "          [550, 127, 9, 12, 2],\n",
    "          [593, 112, 12, 13, 2],\n",
    "          [665, 182, 6, 11, 2]]\n",
    "\n",
    "img_path = \"../wider_face/validation/39--Ice_Skating/39_Ice_Skating_iceskiing_39_583.jpg\"\n",
    "image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "UTILS.plot_image(image, bboxes, yolo=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2ehtU6UTQSW"
   },
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11nGIxVPTO4f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function similar to the one in Yolov3 paper,\n",
    "the difference from what I can tell is I use CrossEntropy for the classes\n",
    "instead of BinaryCrossEntropy.\n",
    "\"\"\"\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        # self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "\n",
    "        # Constants signifying how much to pay for each respective part of the loss\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "        # self.lambda_noobj = 15\n",
    "        # self.lambda_obj = 2\n",
    "        # self.lambda_box = 35\n",
    "\n",
    "    def weighted_mse_loss(self, input, target, weight):\n",
    "        return torch.sum(weight * (input - target) ** 2)\n",
    "\n",
    "    def obj_present(self, target):\n",
    "        return target[..., 4] == 1\n",
    "\n",
    "    def get_box_predictions(self, bbox_list):\n",
    "        return bbox_list[...,0:4]\n",
    "\n",
    "    def get_obj_predictions(self, bbox_list):\n",
    "        return bbox_list[...,4]\n",
    "\n",
    "    def get_no_object_loss(self, target, prediction, obj_present):\n",
    "\n",
    "        obj_not_present = torch.logical_not(obj_present)\n",
    "\n",
    "        target_object_score = self.get_obj_predictions(target[obj_not_present])\n",
    "        prediction_object_score = self.get_obj_predictions(prediction[obj_not_present])\n",
    "\n",
    "        return self.bce(prediction_object_score, target_object_score)\n",
    "\n",
    "    def get_object_loss_simplified(self, target, prediction, object_present):\n",
    "\n",
    "        target_object_score = self.get_obj_predictions(target[object_present])\n",
    "        prediction_oject_score = self.get_obj_predictions(prediction[object_present])\n",
    "\n",
    "        object_loss = self.bce(prediction_oject_score, target_object_score)\n",
    "        return object_loss\n",
    "\n",
    "    def get_object_loss(self, target, prediction, object_present):\n",
    "\n",
    "        #take the scale for later\n",
    "        scale = 1 / target.shape[2]\n",
    "        \n",
    "        #select only the boxes of interest (where there are supposed to be boxes)\n",
    "        target = target[object_present]\n",
    "        prediction = prediction[object_present]\n",
    "        \n",
    "        #scale the x and y position according to the scale to be able to calculate IOU (no need to add the actual grid position here because for the IOU only the relative distance is important)\n",
    "        #rescaling is needed because in the output vector of the model x and y are relative to the grid cell, while with and height are relative to the hole image. These have to be in the same shape to be able to calculate IOU\n",
    "        target[...,0:2] = target[...,0:2] * scale\n",
    "        prediction[...,0:2] = prediction[...,0:2] * scale\n",
    "\n",
    "        target_object_score = self.get_obj_predictions(target)\n",
    "        prediction_oject_score = self.get_obj_predictions(prediction)\n",
    "\n",
    "        #calculate the IOU of the boxes \n",
    "        ious = UTILS.intersection_over_union(self.get_box_predictions(target), self.get_box_predictions(prediction.detach()))\n",
    "        ious = torch.diagonal(ious)\n",
    "        \n",
    "        #if there is an object, the object score should be equal to the IOU of the predicting box and the true box\n",
    "        object_loss = self.bce(prediction_oject_score, ious * target_object_score)\n",
    "\n",
    "        return object_loss\n",
    "\n",
    "        # return self.bce(target_object_score, prediction_oject_score)\n",
    "        # return self.mse(target_object_score, prediction_object_score)\n",
    "\n",
    "    def get_box_loss(self, target, prediction, object_present):\n",
    "        target_box = self.get_box_predictions(target[object_present])\n",
    "        predicted_box = self.get_box_predictions(prediction[object_present])\n",
    "\n",
    "        target_box[...,2:4] = torch.log(1e-16 + target_box[...,2:4])\n",
    "        predicted_box[...,2:4] = torch.log(1e-16 + predicted_box[...,2:4])\n",
    "\n",
    "        return self.mse(predicted_box, target_box)\n",
    "\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # FORMAT: (x, y, w, h, obj)\n",
    "        object_present = self.obj_present(target)\n",
    "\n",
    "        return (\n",
    "            self.lambda_box * self.get_box_loss(target, predictions, object_present)\n",
    "            + self.lambda_obj * self.get_object_loss(target, predictions, object_present)\n",
    "            # + self.lambda_obj * self.get_object_loss_simplified(target, predictions, object_present)\n",
    "            + self.lambda_noobj * self.get_no_object_loss(target, predictions, object_present)\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    "This is the implementation of our custom desined loss function \n",
    "\"\"\"\n",
    "class RescaledPositionalLoss(YoloLoss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lambda_pos = 0.01\n",
    "        self.lambda_size = 1\n",
    "        \n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 5\n",
    "\n",
    "    def get_box_loss(self, target, prediction, object_present):\n",
    "        target_box = self.get_box_predictions(target[object_present])\n",
    "        predicted_box = self.get_box_predictions(prediction[object_present])\n",
    "\n",
    "        #the weights are equal to the reciproke of the dimensions of the box (linear)\n",
    "        weights = 1/target_box[...,2:4].detach()\n",
    "\n",
    "        target_box[...,2:4] = torch.log(1e-16 + target_box[...,2:4])\n",
    "        predicted_box[...,2:4] = torch.log(1e-16 + predicted_box[...,2:4])\n",
    "\n",
    "        size_loss = self.mse(predicted_box[...,2:4], target_box[...,2:4])\n",
    "\n",
    "        position_loss_x =  self.weighted_mse_loss(predicted_box[...,0], target_box[...,0], weights[...,0])\n",
    "        position_loss_y =  self.weighted_mse_loss(predicted_box[...,1], target_box[...,1], weights[...,1])\n",
    "\n",
    "        return self.lambda_pos * (position_loss_x + position_loss_y) + self.lambda_size * size_loss\n",
    "\n",
    "class DirectLoss(YoloLoss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_box_loss(self, target, prediction, object_present):\n",
    "        target_box = self.get_box_predictions(target[object_present])\n",
    "        predicted_box = self.get_box_predictions(prediction[object_present])\n",
    "\n",
    "        # The weights are equal to the reciprocal of the dimensions of the box (linear)\n",
    "        weights = 1/target_box[...,2:4].detach()\n",
    "\n",
    "        target_box[...,2:4] = torch.log(1e-16 + target_box[...,2:4])\n",
    "        predicted_box[...,2:4] = torch.log(1e-16 + predicted_box[...,2:4])\n",
    "\n",
    "        loss = torch.sum(UTILS.distance_intersection_over_union_loss(predicted_box, target_box))\n",
    "        return loss\n",
    "\n",
    "class CompleteLoss(YoloLoss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_box_loss(self, target, prediction, object_present):\n",
    "\n",
    "        target_box = self.get_box_predictions(target[object_present])  # Gecheckt\n",
    "        predicted_box = self.get_box_predictions(prediction[object_present])  # Gecheckt\n",
    "        # The weights are equal to the reciprocal of the dimensions of the box (linear)\n",
    "        weights = 1/target_box[...,2:4].detach()\n",
    "        \n",
    "        target_box[...,2:4] = torch.log(1e-16 + target_box[...,2:4])\n",
    "        predicted_box[...,2:4] = torch.log(1e-16 + predicted_box[...,2:4])\n",
    "        \n",
    "        loss = torch.sum(UTILS.complete_intersection_over_union_loss(predicted_box, target_box))\n",
    "        return loss\n",
    "\n",
    "class GeneralLoss(YoloLoss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_box_loss(self, target, prediction, object_present):\n",
    "        target_box = self.get_box_predictions(target[object_present])\n",
    "        predicted_box = self.get_box_predictions(prediction[object_present])\n",
    "\n",
    "        # The weights are equal to the reciprocal of the dimensions of the box (linear)\n",
    "        weights = 1/target_box[...,2:4].detach()\n",
    "\n",
    "        target_box[...,2:4] = torch.log(1e-16 + target_box[...,2:4])\n",
    "        predicted_box[...,2:4] = torch.log(1e-16 + predicted_box[...,2:4])\n",
    "\n",
    "        loss = torch.sum(UTILS.generalised_intersection_over_union_loss(predicted_box, target_box))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLossFunction(unittest.TestCase):\n",
    "    \n",
    "    def generate_empty_example(self):\n",
    "        batch_size = 1\n",
    "        S = 3\n",
    "        anchors_per_scale = 3\n",
    "\n",
    "        target = torch.zeros(batch_size, anchors_per_scale, S, S, 5)\n",
    "        predictions = torch.zeros(batch_size, anchors_per_scale, S, S, 5)\n",
    "\n",
    "        return target, predictions\n",
    "\n",
    "    def setUp(self):\n",
    "        self.lossfn = YoloLoss()\n",
    "        self.rescaledpositionallossfn = RescaledPositionalLoss()\n",
    "\n",
    "    def test_no_object_loss(self):\n",
    "        target, predictions = self.generate_empty_example()\n",
    "\n",
    "        #correct object score, loss = 0\n",
    "        target[0,0,0,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "        predictions[0,0,0,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "\n",
    "        #incorrect object score, but should be ignored because there is an object loss = 0\n",
    "        target[0,0,0,1] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "        predictions[0,0,0,1] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0.5])\n",
    "\n",
    "         #wrong_box_score, loss = -1 * (yn log(xn) + (1-yn)log(1-xn)) = \n",
    "        target[0,0,1,1] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "        predictions[0,0,1,1] = torch.Tensor([0, 0.5, 0.2, 0.2, 0.5])\n",
    "        \n",
    "        # #wrong_box_score, loss = -1 * (yn log(xn) + (1-yn) log(1-xn)) = -1 * (0 * log(0) + 1 * log(1)) = -1 * log(1)\n",
    "        #  #bce function sees log(0) as -100\n",
    "        target[0,0,1,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "        predictions[0,0,1,0] = torch.Tensor([0, 0.5, 0.2, 0.2, 1])\n",
    "\n",
    "        object_present = self.lossfn.obj_present(target)\n",
    "        no_object_loss = self.lossfn.get_no_object_loss(target, predictions, object_present)\n",
    "\n",
    "        manual_loss = -1 * (1 * math.log(1-0.5) + 1 * -100)/ (3 * 3 * 3 - 1)\n",
    "\n",
    "        self.assertTrue(abs(manual_loss - no_object_loss.item()) < 1e-5)\n",
    "\n",
    "    def test_object_loss(self):\n",
    "        target, predictions = self.generate_empty_example()\n",
    "\n",
    "        #correct object score, IOU = 1, loss = 0\n",
    "        target[0,0,0,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "        predictions[0,0,0,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "\n",
    "        #wrong object score, IOU = 0.5, loss = -1 (0.5log(1) + 0.5 * -100)\n",
    "        target[0,0,0,1] = torch.Tensor([0.1, 0.1, 0.2, 0.2, 1])\n",
    "        predictions[0,0,0,1] = torch.Tensor([0.1, 0.1, 0.1, 0.2, 1])\n",
    "\n",
    "        # #wrong object score, but should be ignored\n",
    "        target[0,0,1,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "        predictions[0,0,1,0] = torch.Tensor([0.5, 0.5, 0.1, 0.2, 1])\n",
    "\n",
    "        # #wrong object score, IOU = 0.25, loss = -1 (0.25log(0.5) + 0.75log(0.5))\n",
    "        target[0,0,1,1] = torch.Tensor([0.9, 0.9, 0.2, 0.2, 1])\n",
    "        predictions[0,0,1,1] = torch.Tensor([0.9, 0.9, 0.2, 0.05, 0.5])\n",
    "\n",
    "        object_present = self.lossfn.obj_present(target)\n",
    "        object_loss = self.lossfn.get_object_loss(target, predictions, object_present)\n",
    "\n",
    "        manual_loss =(-1 * (0.5 * math.log(1) + 0.5 * -100) + -1 * (0.25 * math.log(0.5) + 0.75 * math.log(0.5)))/ 3\n",
    "        \n",
    "        self.assertTrue(abs(manual_loss - object_loss.item()) < 1e-5)\n",
    "\n",
    "    def test_box_loss(self):\n",
    "        target, predictions = self.generate_empty_example()\n",
    "\n",
    "        #correct_box_score, loss = 0\n",
    "        target[0,0,0,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "        predictions[0,0,0,0] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "\n",
    "        #wrong_box_score, loss = 0.5^2/4 = 0.0625\n",
    "        target[0,0,0,1] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "        predictions[0,0,0,1] = torch.Tensor([0, 0.5, 0.2, 0.2, 0])\n",
    "\n",
    "        #wrong_box_score, loss = 0.5^2/4 = 0.0625\n",
    "        target[0,0,1,1] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "        predictions[0,0,1,1] = torch.Tensor([0, 0.5, 0.2, 0.2, 0])\n",
    "\n",
    "        #wrong_box_score, but should be ignored. loss = 0\n",
    "        target[0,0,0,2] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 0])\n",
    "        predictions[0,0,0,2] = torch.Tensor([0, 0.5, 0.2, 0.2, 0])\n",
    "\n",
    "        #wrong_box_score, loss = (log(0.2)-log(0.02))^2 * 2 / 4\n",
    "        target[0,0,1,2] = torch.Tensor([0.5, 0.5, 0.2, 0.2, 1])\n",
    "        predictions[0,0,1,2] = torch.Tensor([0.5, 0.5, 0.02, 0.02, 1])\n",
    "\n",
    "        object_present = self.lossfn.obj_present(target)\n",
    "        box_loss = self.lossfn.get_box_loss(target, predictions, object_present)\n",
    "\n",
    "        #mean loss = 0.0625 + 0.0625 / 3 = 0.625\n",
    "        manual_loss = (0.0625 + 0.0625 + (math.log(0.2) - math.log(0.02))**2 * 2 / 4) / 4\n",
    "\n",
    "        self.assertTrue(abs(box_loss.item() - manual_loss) < 0.0001)\n",
    "\n",
    "    def test_rescaled_positional_loss(self):\n",
    "        target, predictions = self.generate_empty_example()\n",
    "\n",
    "        target[0,0,0,0] = torch.Tensor([0.2, 0.2, 0.2, 0.2, 1])\n",
    "        predictions[0,0,0,0] = torch.Tensor([0.3, 0.3, 0.2, 0.2, 1])\n",
    "\n",
    "        target[0,0,0,1] = torch.Tensor([0.5, 0.5, 0.02, 0.02, 1])\n",
    "        predictions[0,0,0,1] = torch.Tensor([0.1, 0.1, 0.02, 0.02, 1])\n",
    "\n",
    "        object_present = self.lossfn.obj_present(target)\n",
    "        loss1 = self.rescaledpositionallossfn.get_box_loss(target, predictions, object_present)\n",
    "\n",
    "        target, predictions = self.generate_empty_example()\n",
    "\n",
    "        target[0,0,0,0] = torch.Tensor([0.2, 0.2, 0.1, 0.1, 1])\n",
    "        predictions[0,0,0,0] = torch.Tensor([0.3, 0.3, 0.1, 0.1, 1])\n",
    "\n",
    "        target[0,0,0,1] = torch.Tensor([0.5, 0.5, 0.01, 0.01, 1])\n",
    "        predictions[0,0,0,1] = torch.Tensor([0.1, 0.1, 0.01, 0.01, 1])\n",
    "\n",
    "        object_present = self.lossfn.obj_present(target)\n",
    "        loss2 = self.rescaledpositionallossfn.get_box_loss(target, predictions, object_present)\n",
    "\n",
    "        self.assertEqual(loss1.item() * 2, loss2.item())\n",
    "\n",
    "# unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, test_loader, train_loader_evaluation, evaluation_loader = UTILS.get_loaders()\n",
    "# x, y = iter(train_loader_evaluation).next()\n",
    "# bboxes_batch = UTILS.cellvectors_to_bboxes(y, CONFIG.ANCHORS)\n",
    "# bboxes = UTILS.non_max_suppression(\n",
    "#     bboxes_batch[0], iou_threshold=1, threshold=0.7)\n",
    "# # print(bboxes_batch)\n",
    "# bboxes[1][3] = bboxes[1][3]/2\n",
    "# bboxes[2][2] = bboxes[2][2]/2\n",
    "# bboxes[2][3] = bboxes[2][3]/2\n",
    "# bboxes.append(copy.copy(bboxes[2]))\n",
    "# bboxes[3][0] = 0\n",
    "\n",
    "# # print(bboxes)\n",
    "\n",
    "# UTILS.plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), bboxes)\n",
    "\n",
    "# reference = torch.Tensor([bboxes[0][0:4]])\n",
    "# bboxes_tensor = torch.Tensor([box[0:4] for box in bboxes])\n",
    "# print(reference)\n",
    "# print(bboxes_tensor)\n",
    "\n",
    "# print(UTILS.intersection_over_union(reference, bboxes_tensor, \"cxcywh\"))\n",
    "# print(UTILS.intersection_over_union(reference, bboxes_tensor, \"xywh\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for db split code:\n",
    "\n",
    "#train_dataset = WIDER_Dataset(\n",
    "#    CONFIG.INDEX_PATH_TRAIN, \n",
    "#    CONFIG.IMAGE_STORE_PATH_TRAIN, \n",
    "#    transform=CONFIG.test_transforms\n",
    "#)\n",
    "#train_dataset.setup(0, 5)\n",
    "        \n",
    "#train_loader = DataLoader(\n",
    "#    dataset=train_dataset,\n",
    "#    batch_size=CONFIG.BATCH_SIZE,\n",
    "#    num_workers=CONFIG.NUM_WORKERS,\n",
    "#    pin_memory=CONFIG.PIN_MEMORY,\n",
    "#    shuffle=True,\n",
    "#    drop_last=False,\n",
    "#)\n",
    "\n",
    "#img, label = train_dataset[0]\n",
    "#length = train_dataset.__len__()\n",
    "#print(length)\n",
    "\n",
    "#print (img)\n",
    "\n",
    "#UTILS.plot_image(img.permute(1, 2, 0).to(\"cpu\"), label)\n",
    "\n",
    "#trainSplit, valSplit, testSplit = CONFIG.dataSplit\n",
    "#total = CONFIG.numOfIm\n",
    "\n",
    "#trainStart = 0\n",
    "#trainEnd = int(total*trainSplit) -1\n",
    "\n",
    "#valStart = trainEnd+1\n",
    "#valEnd = int(valStart+(total*valSplit)) -1\n",
    "\n",
    "#testStart = valEnd+1\n",
    "#testEnd = int(testStart+(total*testSplit))\n",
    "\n",
    "#print(total)\n",
    "\n",
    "#print(trainStart)\n",
    "#print(trainEnd)\n",
    "#print(valStart)\n",
    "#print(valEnd)\n",
    "#print(testStart)\n",
    "#print(testEnd)\n",
    "\n",
    "\n",
    "#UTILS.plot_image(img.permute(1, 2, 0).to(\"cpu\"), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "CONFIG.DEVICE = 'cpu'\n",
    "model = YOLOv3().to(CONFIG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_lr = 1e-3\n",
    "anchors = CONFIG.ANCHORS\n",
    "\n",
    "# REMOVE\n",
    "\n",
    "def train_model(model, train_loader, epochs):\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=test_lr, weight_decay=0\n",
    "    )\n",
    "\n",
    "    loss_fn = GeneralLoss()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i, (x, y) in enumerate(train_loader_evaluation):\n",
    "            x = x.to(CONFIG.DEVICE)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x)\n",
    "            out_cpu = [output.to(\"cpu\") for output in out]\n",
    "\n",
    "            loss =  loss_fn(out[0], y[0].to(CONFIG.DEVICE))\\\n",
    "                    + loss_fn(out[1], y[1].to(CONFIG.DEVICE))\\\n",
    "                    + loss_fn(out[2], y[2].to(CONFIG.DEVICE))\n",
    "                \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "\n",
    "                bboxes_batch = UTILS.cellvectors_to_bboxes(out_cpu, CONFIG.ANCHORS)\n",
    "\n",
    "                for i in range(len(x)):\n",
    "\n",
    "                    bboxes = UTILS.non_max_suppression(bboxes_batch[i], threshold=0.7)\n",
    "                    UTILS.plot_image(x[i].permute(1, 2, 0).to(\"cpu\"), bboxes)\n",
    "\n",
    "                    # print(\"predicted boxvectors:\")\n",
    "                    # print(out_cpu[0][i][loss_fn.obj_present(y[0][i])])\n",
    "                    # print(\"target boxvectors:\")\n",
    "                    # print(y[0][i][loss_fn.obj_present(y[0][i])])\n",
    "\n",
    "                    print(\"Loss: {}\".format(loss))\n",
    "\n",
    "                mean_ap = UTILS.evaluate_model(train_loader_evaluation, model)\n",
    "                print(\"mAP: {}\".format(mean_ap))\n",
    "\n",
    "train_loader, test_loader, train_loader_evaluation = UTILS.get_loaders()\n",
    "        \n",
    "train_model(model, train_loader, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKTKFfHnTYsx"
   },
   "source": [
    "# Train Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If had a previous run, you can set a number of previous epochs when loadnig from a checkpoint\n",
    "def train_model(model, loss_fn, optimizer, train_loader, test_loader,\n",
    "                epochs=CONFIG.NUM_EPOCHS, load_checkpoint = CONFIG.LOAD_CHECKPOINT, save_model = CONFIG.SAVE_MODEL, save_results = CONFIG.SAVE_RESULTS,\n",
    "                verbose = False, show_examples = False, early_stop = False, stop_plateau = False, loss_margin = 0.001):\n",
    "    early_save = False\n",
    "    resume_epoch = 0\n",
    "    old_loss = float('-inf')\n",
    "    best_map = 0\n",
    "    if load_checkpoint:\n",
    "        resume_epoch,old_loss,best_map = load_model(model,loss_fn,optimizer,verbose=verbose)\n",
    "\n",
    "    # Progress bars for epoch and batch counting\n",
    "    epoch_tqdm = trange(epochs, desc=\"Epochs\", initial=resume_epoch)\n",
    "    batch_tqdm = trange(len(train_loader), desc=\"Batches\")\n",
    "\n",
    "    for epoch in epoch_tqdm:\n",
    "        # reset batch progress bar at the start of each epoch\n",
    "        batch_tqdm.reset()\n",
    "\n",
    "        all_loss = []\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # update the batch progress bar\n",
    "            batch_tqdm.update(1)\n",
    "\n",
    "            x = x.to(CONFIG.DEVICE)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(x)\n",
    "\n",
    "            out_cpu = [output.to(\"cpu\") for output in out]\n",
    "\n",
    "            loss = (\n",
    "                    loss_fn(out[0], y[0].to(CONFIG.DEVICE))\n",
    "                    + loss_fn(out[1], y[1].to(CONFIG.DEVICE))\n",
    "                    + loss_fn(out[2], y[2].to(CONFIG.DEVICE))\n",
    "            )\n",
    "\n",
    "            all_loss.append(float(loss))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if show_examples and i % 200 == 0 and i != 0:\n",
    "                bboxes_batch = UTILS.cellvectors_to_bboxes(out_cpu, CONFIG.ANCHORS)\n",
    "                bboxes = UTILS.non_max_suppression(bboxes_batch[0], threshold=0.5)\n",
    "\n",
    "                UTILS.plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), bboxes)\n",
    "\n",
    "                print(\"predicted boxvectors:\")\n",
    "                print(out_cpu[0][loss_fn.obj_present(y[0])])\n",
    "                print(\"target boxvectors:\")\n",
    "                print(y[0][loss_fn.obj_present(y[0])])\n",
    "\n",
    "                if CONFIG.DEVICE == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "        average_loss = sum(all_loss)/len(all_loss)\n",
    "        map = UTILS.evaluate_model(test_loader, model)\n",
    "\n",
    "        if map > best_map:\n",
    "            best_map = map\n",
    "            if early_stop:\n",
    "                early_save = True\n",
    "\n",
    "        line = \"Epoch: {}, Loss: {}, MAP: {}, Best_MAP: {}\".format(epoch+resume_epoch, average_loss, map, best_map)\n",
    "        if verbose:\n",
    "            print(line)\n",
    "\n",
    "        if save_results:\n",
    "            results_path = \"{}{}_{}.txt\".format(CONFIG.RESULTS_PATH, model.__class__.__name__, loss_fn.__class__.__name__)\n",
    "            if os.path.exists(results_path):\n",
    "                with open(results_path, \"a\") as text_file:\n",
    "                    print(line, file=text_file)\n",
    "            else:\n",
    "                if not os.path.exists(CONFIG.RESULTS_PATH):\n",
    "                    os.mkdir(CONFIG.RESULTS_PATH)\n",
    "                with open(results_path, \"x\") as text_file:\n",
    "                    print(line, file=text_file)\n",
    "        \n",
    "        if save_model:\n",
    "            if not os.path.exists(CONFIG.CHECKPOINT_PATH):\n",
    "                os.mkdir(CONFIG.CHECKPOINT_PATH)\n",
    "            model_path = \"{}{}_{}.tar\".format(CONFIG.CHECKPOINT_PATH, model.__class__.__name__, loss_fn.__class__.__name__)\n",
    "            UTILS.save_checkpoint(model, optimizer, filename=model_path)\n",
    "            if early_save:\n",
    "                early_save = False\n",
    "                model_path_best = \"{}{}_{}_best.tar\".format(CONFIG.CHECKPOINT_PATH, model.__class__.__name__, loss_fn.__class__.__name__)\n",
    "                UTILS.save_checkpoint(model, optimizer, filename=model_path_best)\n",
    "\n",
    "\n",
    "        if stop_plateau and average_loss < old_loss and (average_loss + average_loss*loss_margin) < old_loss:\n",
    "            print(\"plateau reached, training will be stopped\")\n",
    "            print(\"Current loss: {}, previous loss: {}\".format((average_loss + average_loss*loss_margin),old_loss))\n",
    "            return model\n",
    "        else:\n",
    "            old_loss = average_loss\n",
    "\n",
    "        batch_tqdm.refresh()\n",
    "\n",
    "def load_model(model,loss_fn,optimizer,verbose = False):\n",
    "    model_path = \"{}{}_{}.tar\".format(CONFIG.CHECKPOINT_PATH, model.__class__.__name__, loss_fn.__class__.__name__)\n",
    "    if os.path.exists(model_path):\n",
    "        UTILS.load_checkpoint(model_path,model,optimizer,CONFIG.LEARNING_RATE)\n",
    "        if CONFIG.DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    results_path = \"{}{}_{}.txt\".format(CONFIG.RESULTS_PATH, model.__class__.__name__, loss_fn.__class__.__name__)\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'r') as file:\n",
    "            lastline=file.readlines()[-1]\n",
    "            if verbose:\n",
    "                print(\"Last checkpoint:\")\n",
    "                print(lastline)\n",
    "            resume_epoch = int(lastline.split(',')[0].split(':')[1])+1\n",
    "            loss = float(lastline.split(',')[1].split(':')[1])\n",
    "            best_map = float(lastline.split(',')[3].split(':')[1])\n",
    "            return resume_epoch, loss, best_map\n",
    "    return 0, float('-inf'), 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = YOLOv3().to(CONFIG.DEVICE)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=CONFIG.LEARNING_RATE, weight_decay=CONFIG.WEIGHT_DECAY\n",
    ")\n",
    "train_loader, test_loader, train_loader_evaluation, validation_loader = UTILS.get_loaders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_model(model, YoloLoss(), optimizer, train_loader,\n",
    "            test_loader, verbose=True, early_stop=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "YOLOv3",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8b5673476864a226669bfdfde40cbe13c64fc2596722f9e235eba04a6cd6cf16"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
