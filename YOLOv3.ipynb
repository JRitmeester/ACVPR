{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv3",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit (conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "44dc130470a3076819252787b33dc1d71b2ba6a95ec364c545cf20092901b262"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hdbXoXQTAoX"
      },
      "source": [
        "# YOLOv3 Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFgQWo3RRbPH"
      },
      "source": [
        "\"\"\"\n",
        "Implementation of YOLOv3 architecture\n",
        "\"\"\"\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        " \n",
        "\"\"\" \n",
        "Information about architecture config:\n",
        "Tuple is structured by (filters, kernel_size, stride) \n",
        "Every conv is a same convolution. \n",
        "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
        "\"S\" is for scale prediction block and computing the yolo loss\n",
        "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
        "\"\"\"\n",
        "#G: \"P\" indicates a concatination point for the feature pyramid\n",
        "\n",
        "config = [\n",
        "    (32, 3, 1),\n",
        "    (64, 3, 2),\n",
        "    [\"B\", 1],\n",
        "    \"P\",\n",
        "    (128, 3, 2),\n",
        "    [\"B\", 2],\n",
        "    (256, 3, 2),\n",
        "    [\"B\", 8],\n",
        "    \"P\",\n",
        "    (512, 3, 2), #10\n",
        "    [\"B\", 8],\n",
        "    (1024, 3, 2),\n",
        "    [\"B\", 4],  # To this point is Darknet-53\n",
        "    (512, 1, 1),\n",
        "    (1024, 3, 1),\n",
        "    \"S\",\n",
        "    #(256, 1, 1), #original\n",
        "    (64, 1, 1),\n",
        "    \"U\", #20\n",
        "    (256, 1, 1),\n",
        "    (512, 3, 1),\n",
        "    \"S\",\n",
        "    #(128, 1, 1), #original\n",
        "    (256, 1, 1),\n",
        "    \"U\", #26\n",
        "    (128, 1, 1),\n",
        "    (256, 3, 1),\n",
        "    \"S\",\n",
        "]\n",
        " \n",
        "class ConcatBlock(nn.Module): #just passes through the previous layer, is just used as a indicator for what layer sould be concatinated with the feature pyramid.\n",
        "    def _init_():\n",
        "        super()._init_()\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.1)\n",
        "        self.use_bn_act = bn_act\n",
        " \n",
        "    def forward(self, x):\n",
        "        if self.use_bn_act:\n",
        "            return self.leaky(self.bn(self.conv(x)))\n",
        "        else:\n",
        "            return self.conv(x)\n",
        " \n",
        " \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for repeat in range(num_repeats):\n",
        "            self.layers += [\n",
        "                nn.Sequential(\n",
        "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
        "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
        "                )\n",
        "            ]\n",
        " \n",
        "        self.use_residual = use_residual\n",
        "        self.num_repeats = num_repeats\n",
        " \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            if self.use_residual:\n",
        "                x = x + layer(x) #go through each residual block (2 conv layers and a residual layer)\n",
        "            else:\n",
        "                x = layer(x)\n",
        " \n",
        "        return x\n",
        " \n",
        " \n",
        "class ScalePrediction(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.pred = nn.Sequential(\n",
        "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
        "            CNNBlock(\n",
        "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
        "            ),\n",
        "        )\n",
        "        self.num_classes = num_classes\n",
        " \n",
        "    def forward(self, x):\n",
        "        return (\n",
        "            self.pred(x)\n",
        "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
        "            .permute(0, 1, 3, 4, 2)\n",
        "        )\n",
        " \n",
        " \n",
        "class YOLOv3(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=80):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.in_channels = in_channels\n",
        "        self.layers = self._create_conv_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #count=0\n",
        "        outputs = []  # for each scale\n",
        "        route_connections = []\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, ScalePrediction):\n",
        "                outputs.append(layer(x))\n",
        "                continue\n",
        "            #count += 1\n",
        "            #print(\"count is \"+str(count))\n",
        "            x = layer(x)\n",
        " \n",
        "      #      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8: #store this layer for the feature pyramid\n",
        "            if isinstance(layer, ConcatBlock):\n",
        "                route_connections.append(x)\n",
        " \n",
        "            elif isinstance(layer, nn.Upsample): \n",
        "                x = torch.cat([x, route_connections[-1]], dim=1)#cat. the layers after each upsample to create the feature pyramid\n",
        "                route_connections.pop()\n",
        " \n",
        "        return outputs\n",
        " \n",
        "    def _create_conv_layers(self):\n",
        "        layers = nn.ModuleList()\n",
        "        in_channels = self.in_channels\n",
        " \n",
        "        for module in config:\n",
        "            if isinstance(module, tuple):\n",
        "                out_channels, kernel_size, stride = module\n",
        "                layers.append(\n",
        "                    CNNBlock(\n",
        "                        in_channels,\n",
        "                        out_channels,\n",
        "                        kernel_size=kernel_size,\n",
        "                        stride=stride,\n",
        "                        padding=1 if kernel_size == 3 else 0,\n",
        "                    )\n",
        "                )\n",
        "                in_channels = out_channels\n",
        " \n",
        "            elif isinstance(module, list):\n",
        "                num_repeats = module[1]\n",
        "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
        " \n",
        "            elif isinstance(module, str):\n",
        "                if module == \"S\":\n",
        "                    layers += [\n",
        "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
        "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
        "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
        "                    ]\n",
        "                    in_channels = in_channels // 2\n",
        " \n",
        "                elif module == \"U\":\n",
        "                    layers.append(nn.Upsample(scale_factor=4),)\n",
        "                    #in_channels = in_channels * 5 #original\n",
        "                    in_channels = 320 #this number seems to work (based on the number of channels in the error message)\n",
        "                elif module == \"P\":\n",
        "                    layers.append(ConcatBlock())\n",
        "                    in_channels = in_channels\n",
        " \n",
        "        return layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdp_rh03TzlV"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsmMjUEYTyKe"
      },
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "import torch\n",
        " \n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class Config:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.DATA_PATH = '../wider_face/'\n",
        "    self.INDEX_PATH_TRAIN = self.DATA_PATH + 'annotation/wider_face_train_bbx_gt.txt'\n",
        "    self.INDEX_PATH_VAL = self.DATA_PATH + 'annotation/wider_face_val_bbx_gt.txt'\n",
        "    self.IMAGE_STORE_PATH_TRAIN = self.DATA_PATH + 'train/'\n",
        "    self.IMAGE_STORE_PATH_VAL = self.DATA_PATH + 'validation/'\n",
        "    self.ANCHORS = [\n",
        "      [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "      [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "      [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "    ]  # Note these have been rescaled to be between [0, 1]\n",
        "  \n",
        "    self.DATASET = '/PASCAL_VOC'\n",
        "    self.IMG_DIR = '/PASCAL_VOC/images/'\n",
        "    self.LABEL_DIR = '/PASCAL_VOC/labels/'\n",
        "    self.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # UTILS.seed_everything()  # If you want deterministic behavior\n",
        "    self.NUM_WORKERS = 0 #4 causes errors on windows to have more than 0\n",
        "    self.BATCH_SIZE = 32\n",
        "    self.IMAGE_SIZE = 416\n",
        "    self.NUM_CLASSES = 2 #face or no face\n",
        "    self.LEARNING_RATE = 1e-4\n",
        "    self.WEIGHT_DECAY = 0 #1e-4 staat uit nu om te overfitten, kan aan tijdens echte trainen\n",
        "    self.NUM_EPOCHS = 100\n",
        "    self.CONF_THRESHOLD = 0.05\n",
        "    self.MAP_IOU_THRESH = 0.5\n",
        "    self.NMS_IOU_THRESH = 0.45\n",
        "  #  self.S = [self.IMAGE_SIZE // 32, self.IMAGE_SIZE // 16, self.IMAGE_SIZE // 8]\n",
        "    self.S = [self.IMAGE_SIZE // 32, self.IMAGE_SIZE // 8, self.IMAGE_SIZE // 2]\n",
        "    self.PIN_MEMORY = True\n",
        "    self.LOAD_MODEL = False\n",
        "    self.SAVE_MODEL = True\n",
        "    self.CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
        "    \n",
        "    self.ANCHORS = [\n",
        "        [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "        [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "        [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "    ]  # Note these have been rescaled to be between [0, 1]\n",
        "    \n",
        "    self.scale = 1.1\n",
        "    self.train_transforms = A.Compose(\n",
        "        [\n",
        "            A.LongestMaxSize(max_size=int(self.IMAGE_SIZE * self.scale)),\n",
        "            A.PadIfNeeded(\n",
        "                min_height=int(self.IMAGE_SIZE * self.scale),\n",
        "                min_width=int(self.IMAGE_SIZE * self.scale),\n",
        "                border_mode=cv2.BORDER_CONSTANT,\n",
        "            ),\n",
        "            A.RandomCrop(width=self.IMAGE_SIZE, height=self.IMAGE_SIZE),\n",
        "            A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Blur(p=0.1),\n",
        "            A.CLAHE(p=0.1),\n",
        "            A.Posterize(p=0.1),\n",
        "            A.ToGray(p=0.1),\n",
        "            A.ChannelShuffle(p=0.05),\n",
        "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
        "    )\n",
        "\n",
        "    self.test_transforms = A.Compose(\n",
        "        [\n",
        "            A.LongestMaxSize(max_size=self.IMAGE_SIZE),\n",
        "            A.PadIfNeeded(\n",
        "                min_height=self.IMAGE_SIZE, min_width=self.IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
        "            ),\n",
        "            A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
        "    )\n",
        "\n",
        "CONFIG = Config()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CcNuKTNUCxn"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqeOM9HNUEug"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torchvision\n",
        " \n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision.ops import nms\n",
        "from torchvision.ops import box_iou\n",
        "\n",
        "class Utils():\n",
        " \n",
        "  def iou_width_height(self, boxes1, boxes2):\n",
        "      \"\"\"\n",
        "      Parameters:\n",
        "          boxes1 (tensor): width and height of the first bounding boxes\n",
        "          boxes2 (tensor): width and height of the second bounding boxes\n",
        "      Returns:\n",
        "          tensor: Intersection over union of the corresponding boxes\n",
        "      \"\"\"\n",
        "      intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "          boxes1[..., 1], boxes2[..., 1]\n",
        "      )\n",
        "      union = (\n",
        "          boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "      )\n",
        "      return intersection / union\n",
        "  \n",
        "  \n",
        "  def intersection_over_union(self, boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "      \"\"\"\n",
        "      Video explanation of this function:\n",
        "      https://youtu.be/XXYG5ZWtjj0\n",
        "      This function calculates intersection over union (iou) given pred boxes\n",
        "      and target boxes.\n",
        "      Parameters:\n",
        "          boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "          boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "          box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
        "      Returns:\n",
        "          tensor: Intersection over union for all examples\n",
        "      \"\"\"\n",
        "      if box_format == \"midpoint\":\n",
        "          box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "          box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "          box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "          box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "          box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "          box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "          box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "          box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "  \n",
        "      if box_format == \"corners\":\n",
        "          box1_x1 = boxes_preds[..., 0:1]\n",
        "          box1_y1 = boxes_preds[..., 1:2]\n",
        "          box1_x2 = boxes_preds[..., 2:3]\n",
        "          box1_y2 = boxes_preds[..., 3:4]\n",
        "          box2_x1 = boxes_labels[..., 0:1]\n",
        "          box2_y1 = boxes_labels[..., 1:2]\n",
        "          box2_x2 = boxes_labels[..., 2:3]\n",
        "          box2_y2 = boxes_labels[..., 3:4]\n",
        "  \n",
        "      x1 = torch.max(box1_x1, box2_x1)\n",
        "      y1 = torch.max(box1_y1, box2_y1)\n",
        "      x2 = torch.min(box1_x2, box2_x2)\n",
        "      y2 = torch.min(box1_y2, box2_y2)\n",
        "  \n",
        "      intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "      box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "      box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "  \n",
        "      return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "  \n",
        "  \n",
        "  def non_max_suppression(self, bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "      \"\"\"\n",
        "      Video explanation of this function:\n",
        "      https://youtu.be/YDkjWEN8jNA\n",
        "      Does Non Max Suppression given bboxes\n",
        "      Parameters:\n",
        "          bboxes (list): list of lists containing all bboxes with each bboxes\n",
        "          specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "          iou_threshold (float): threshold where predicted bboxes is correct\n",
        "          threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
        "          box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "      Returns:\n",
        "          list: bboxes after performing NMS given a specific IoU threshold\n",
        "      \"\"\"\n",
        "  \n",
        "      assert type(bboxes) == list\n",
        "  \n",
        "      bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "      \n",
        "      # bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "      # bboxes_after_nms = []\n",
        "  \n",
        "      # while bboxes:\n",
        "      #     chosen_box = bboxes.pop(0)\n",
        "      #     bboxes = [\n",
        "      #         box\n",
        "      #         for box in bboxes\n",
        "      #         if box[0] != chosen_box[0]\n",
        "      #         or self.intersection_over_union(\n",
        "      #             torch.tensor(chosen_box[2:]),\n",
        "      #             torch.tensor(box[2:]),\n",
        "      #             box_format=box_format\n",
        "      #         )\n",
        "      #         < iou_threshold\n",
        "      #     ]\n",
        "  \n",
        "      #     bboxes_after_nms.append(chosen_box)\n",
        "      # return bboxes_after_nms\n",
        "      boxes = self.box_xywh_to_xyxy(torch.tensor([box[2:] for box in bboxes]).to(CONFIG.DEVICE))\n",
        "      scores = torch.tensor([box[1] for box in bboxes]).to(CONFIG.DEVICE)\n",
        "\n",
        "      return [bboxes[i] for i in nms(boxes, scores, iou_threshold).tolist()]\n",
        "  \n",
        "  def box_xywh_to_xyxy(self, boxes : torch.Tensor):\n",
        "      # Idea is to support all boxes and do processing in batch. Same as done in box_area\n",
        "      xyxy = torch.Tensor(boxes.shape).to(CONFIG.DEVICE)\n",
        "      xyxy[:, 0] = boxes[:, 0] - 0.5*boxes[:, 2]  # x - 0.5w\n",
        "      xyxy[:, 1] = boxes[:, 1] - 0.5*boxes[:, 3]  # y - 0.5h\n",
        "      xyxy[:, 2] = boxes[:, 0] + 0.5*boxes[:, 2]  # x + 0.5w\n",
        "      xyxy[:, 3] = boxes[:, 1] + 0.5*boxes[:, 3]  # y + 0.5h\n",
        "      return xyxy\n",
        "\n",
        "  \n",
        "\n",
        "  def mean_average_precision(self, pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Video explanation of this function:\n",
        "      https://youtu.be/FppOzcDvaDI\n",
        "      This function calculates mean average precision (mAP)\n",
        "      Parameters:\n",
        "          pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "          specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "          true_boxes (list): Similar as pred_boxes except all the correct ones\n",
        "          iou_threshold (float): threshold where predicted bboxes is correct\n",
        "          box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "          num_classes (int): number of classes\n",
        "      Returns:\n",
        "          float: mAP value across all classes given a specific IoU threshold\n",
        "      \"\"\"\n",
        "  \n",
        "      # List storing all AP for respective classes\n",
        "      average_precisions = []\n",
        "  \n",
        "      # Used for numerical stability later on.\n",
        "      epsilon = 1e-6\n",
        "  \n",
        "      for c in range(num_classes):\n",
        "          detections = []\n",
        "          ground_truths = []\n",
        "  \n",
        "          # Go through all predictions and targets, and only add the ones that \n",
        "          # belong to the current class c.\n",
        "          for detection in pred_boxes:\n",
        "              if detection[1] == c:\n",
        "                  detections.append(detection)\n",
        "  \n",
        "          for true_box in true_boxes:\n",
        "              if true_box[1] == c:\n",
        "                  ground_truths.append(true_box)\n",
        "  \n",
        "          # Find the amount of bboxes for each training example\n",
        "          # Counter here finds how many ground truth bboxes we get.\n",
        "          # For each training example, so let's say img 0 has 3,\n",
        "          # img 1 has 5 then we will obtain a dictionary with:\n",
        "          # amount_bboxes = {0:3, 1:5}\n",
        "          amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "  \n",
        "          # We then go through each key, val in this dictionary\n",
        "          # and convert to the following (w.r.t same example):\n",
        "          # amount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "          for key, val in amount_bboxes.items():\n",
        "              amount_bboxes[key] = torch.zeros(val)\n",
        "  \n",
        "          # sort by box probabilities which is index 2\n",
        "          detections.sort(key=lambda x: x[2], reverse=True)\n",
        "          TP = torch.zeros((len(detections)))\n",
        "          FP = torch.zeros((len(detections)))\n",
        "          total_true_bboxes = len(ground_truths)\n",
        "  \n",
        "          # If none exists for this class then we can safely skip\n",
        "          if total_true_bboxes == 0:\n",
        "              continue\n",
        "  \n",
        "          for detection_idx, detection in enumerate(detections):\n",
        "              # Only take out the ground_truths that have the same\n",
        "              # training idx as detection\n",
        "              ground_truth_img = [\n",
        "                  bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "              ]\n",
        "  \n",
        "              num_gts = len(ground_truth_img)\n",
        "              best_iou = 0\n",
        "  \n",
        "              # for idx, gt in enumerate(ground_truth_img):\n",
        "              #     # iou = self.intersection_over_union(\n",
        "              #     #     torch.tensor(detection[3:]),\n",
        "              #     #     torch.tensor(gt[3:]),\n",
        "              #     #     box_format=box_format,\n",
        "              #     # )\n",
        "  \n",
        "              #     if iou > best_iou:\n",
        "              #         best_iou = iou\n",
        "              #         best_gt_idx = idx\n",
        "              # print(\"detection: {}\".format(detection))\n",
        "\n",
        "              detection_tensor = torch.Tensor(detection[3:]).to(CONFIG.DEVICE).unsqueeze(0)\n",
        "              detection_tensor = self.box_xywh_to_xyxy(detection_tensor)\n",
        "              ground_truth_tensors = torch.Tensor(ground_truth_img)[:,3:].to(CONFIG.DEVICE)\n",
        "              ground_truth_tensors = self.box_xywh_to_xyxy(ground_truth_tensors)\n",
        "              \n",
        "              iou_matrix = box_iou(detection_tensor,ground_truth_tensors)\n",
        "\n",
        "              best_gt_idx = torch.argmax(iou_matrix)\n",
        "              best_iou = torch.max(iou_matrix)\n",
        "\n",
        "              if best_iou > iou_threshold:\n",
        "                  # Only detect ground truth detection once\n",
        "                  if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                      # true positive and add this bounding box to seen\n",
        "                      TP[detection_idx] = 1\n",
        "                      amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                  else:\n",
        "                      FP[detection_idx] = 1\n",
        "  \n",
        "              # If IOU is lower then the detection is a false positive\n",
        "              else:\n",
        "                  FP[detection_idx] = 1\n",
        "  \n",
        "          TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "          FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "          recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "          precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "          precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "          recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "          # torch.trapz for numerical integration\n",
        "          average_precisions.append(torch.trapz(precisions, recalls))\n",
        "  \n",
        "      return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "  def mean_average_precision2(self, pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=CONFIG.NUM_CLASSES\n",
        "    ):\n",
        "      \"\"\"\n",
        "      Video explanation of this function:\n",
        "      https://youtu.be/FppOzcDvaDI\n",
        "      This function calculates mean average precision (mAP)\n",
        "      Parameters:\n",
        "          pred_boxes (list): list of list of lists containing all bboxes with each bboxes\n",
        "          specified as [class_prediction, prob_score, x1, y1, x2, y2] for each training example\n",
        "          true_boxes (list): Similar as pred_boxes except all the correct ones\n",
        "          iou_threshold (float): threshold where predicted bboxes is correct\n",
        "          box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "          num_classes (int): number of classes\n",
        "      Returns:\n",
        "          float: mAP value across all classes given a specific IoU threshold\n",
        "      \"\"\"\n",
        "\n",
        "      # Used to avoid devide by 0\n",
        "      epsilon = 1e-6\n",
        "\n",
        "      # Convert list of lists of bounding boxes per sample to one list of bounding boxes with the sample as item of a bounding box.\n",
        "      # This allows sorting based on confidence level.\n",
        "      pred_boxes = [[sample_idx] + bbox for sample_idx, sample in enumerate(pred_boxes) for bbox in sample]\n",
        "      true_boxes = [[sample_idx] + bbox for sample_idx, sample in enumerate(true_boxes) for bbox in sample]\n",
        "      \n",
        "      mAP = 0\n",
        "\n",
        "      # Calculate AP per class.\n",
        "      for c in range(num_classes):\n",
        "\n",
        "        # Seperate classes\n",
        "        detections = [bbox for bbox in pred_boxes if bbox[1] == c]\n",
        "        ground_truths = [bbox for bbox in true_boxes if bbox [1] == c]\n",
        "\n",
        "        # If there are no detections for this class, go to the next class.\n",
        "        if len(ground_truths) <= 0:\n",
        "           continue\n",
        "\n",
        "        # Tensor to keep track of already found bounding boxes.\n",
        "        found = torch.zeros(len(ground_truths))\n",
        "\n",
        "        # Sort detections based on confidence, this is the ranking for mAP.\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "        # Make list of TP and FP to keep track of them for each step.\n",
        "        TP = torch.zeros(len(detections))\n",
        "        FP = torch.zeros(len(detections))\n",
        "\n",
        "        # Loop trough the sorted detections starting from highest confidence (rank).\n",
        "        for rank, detection in enumerate(detections):\n",
        "\n",
        "          # Find all possible ground_truths for this detection.\n",
        "          ground_truth = []\n",
        "          ground_truth_id = []\n",
        "          for id, bbox in enumerate(ground_truths):\n",
        "            if bbox[0] == detection[0] and not found[id]:\n",
        "              ground_truth.append(bbox)\n",
        "              ground_truth_id.append(id)\n",
        "\n",
        "          ground_truth = torch.tensor(ground_truth)\n",
        "\n",
        "          # Add dimension to detection for the torchvision box_iou function.\n",
        "          detection = torch.Tensor(detection[-4:]).unsqueeze(0)\n",
        "\n",
        "          print(\"ground_truth: {}\".format(ground_truth))\n",
        "          print(\"detection: {}\".format(detection))\n",
        "\n",
        "          IOU = torchvision.ops.box_iou(detection, ground_truth)\n",
        "          print(\"IOU matrix: {}\".format(IOU))\n",
        "\n",
        "          best_iou = torch.max(IOU)\n",
        "          best_iou_idx = torch.argmax(IOU)\n",
        "\n",
        "          if best_iou >= iou_threshold:\n",
        "            found[ground_truth_id[best_iou_idx]] = 1\n",
        "            TP[rank] = 1\n",
        "          else:\n",
        "            FP[rank] = 1\n",
        "\n",
        "        # Cumulative sum of tp and fp to be able to calculate precision and recall at each rank.\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "\n",
        "        # Precision: how many of the targets you predicted are predicted *correctly*?\n",
        "        # Recall: how many of the targets that you should have found, have you found?\n",
        "        # Calculate recall and precision at each rank\n",
        "        recalls = TP_cumsum / len(ground_truths) #TP / (TP + FN)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)  #TP / TP + FP\n",
        "\n",
        "        # Add 1 to precision because before rank 0 the precision is 100%.\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "\n",
        "        # Add 0 to recalls because before rank 0 the recall is 0%.\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "\n",
        "        # Get area under precision-recall graph using trapezoid rule.\n",
        "        AP = torch.trapz(precisions, recalls)\n",
        "\n",
        "        mAP += AP * len(ground_truth)\n",
        "\n",
        "\n",
        "      mAP = mAP / len(groud_truths)\n",
        "      return mAP\n",
        "\n",
        "\n",
        "  def mean_average_precision3(self, pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=CONFIG.NUM_CLASSES\n",
        "    ):\n",
        "      \"\"\"\n",
        "      Video explanation of this function:\n",
        "      https://youtu.be/FppOzcDvaDI\n",
        "      This function calculates mean average precision (mAP)\n",
        "      Parameters:\n",
        "          pred_boxes (list): list of list of lists containing all bboxes with each bboxes\n",
        "          specified as [class_prediction, prob_score, x1, y1, x2, y2] for each training example\n",
        "          true_boxes (list): Similar as pred_boxes except all the correct ones\n",
        "          iou_threshold (float): threshold where predicted bboxes is correct\n",
        "          box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "          num_classes (int): number of classes\n",
        "      Returns:\n",
        "          float: mAP value across all classes given a specific IoU threshold\n",
        "      \"\"\"\n",
        "\n",
        "      print(\"calculating mAP:\")\n",
        "\n",
        "      # Convert list of lists of bounding boxes per sample to one list of \n",
        "      # bounding boxes with the sample as item of a bounding box.\n",
        "      # This allows sorting based on confidence level\n",
        "      pred_boxes = [[sample_idx] + bbox for sample_idx, sample in enumerate(pred_boxes) for bbox in sample]\n",
        "      true_boxes = [[sample_idx] + bbox for sample_idx, sample in enumerate(true_boxes) for bbox in sample]\n",
        "\n",
        "      # Tensor to keep track of already found bounding boxes.\n",
        "      found = torch.zeros(len(true_boxes))\n",
        "\n",
        "      # Sort detections based on confidence, this is the ranking for mAP\n",
        "      pred_boxes.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "      # Make list of TP and FP to keep track of them for each step\n",
        "      TP = torch.zeros(len(pred_boxes))\n",
        "      FP = torch.zeros(len(pred_boxes))\n",
        "\n",
        "      # Loop trough the sorted detections starting from highest confidence (rank)\n",
        "      for rank, detection in enumerate(tqdm(pred_boxes)):\n",
        "\n",
        "        # Find all possible ground_truths for this detection\n",
        "        ground_truth = []\n",
        "        ground_truth_id = []\n",
        "        for id, bbox in enumerate(true_boxes):\n",
        "          if bbox[0] == detection[0] and bbox[1] == detection[1] and not found[id]:\n",
        "            ground_truth.append(bbox[-4:])\n",
        "            ground_truth_id.append(id)\n",
        "\n",
        "        ground_truth = torch.tensor(ground_truth)\n",
        "\n",
        "        # Add dimension to detection for the torchvision box_iou function\n",
        "        detection = torch.Tensor(detection[-4:]).unsqueeze(0)\n",
        "\n",
        "        # if there is no ground truth, false positive by default\n",
        "        if ground_truth.shape[0] <= 0:\n",
        "          FP[rank] = 1\n",
        "          continue\n",
        "\n",
        "        IOU = torchvision.ops.box_iou(detection, ground_truth)\n",
        "        # print(\"IOU matrix: {}\".format(IOU))\n",
        "\n",
        "        best_iou = torch.max(IOU)\n",
        "        \n",
        "        if rank < 10:\n",
        "          self.plot_boxes(detection.tolist(), ground_truth.tolist())\n",
        "          print(\"detetion: {}\".format(detection))\n",
        "          print(\"ground_truth: {}\".format(ground_truth))\n",
        "          print(\"iou_tensor: {}\".format(IOU))\n",
        "          print(\"best_iou: {}, rank: {}\".format(best_iou, rank))\n",
        "\n",
        "        best_iou_idx = torch.argmax(IOU)\n",
        "\n",
        "        if best_iou >= iou_threshold:\n",
        "          found[ground_truth_id[best_iou_idx]] = 1\n",
        "          TP[rank] = 1\n",
        "        else:\n",
        "          FP[rank] = 1\n",
        "\n",
        "      #comulative sum of tp and fp to be able to calculate precision and recall at each rank\n",
        "      TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "      FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "    \n",
        "      #calculate recall and precision at each rank\n",
        "      recalls = TP_cumsum / len(true_boxes) #TP / (TP + FN)\n",
        "      precisions = TP_cumsum / (TP_cumsum + FP_cumsum)  #TP / TP + FP\n",
        "\n",
        "      #add 1 to precision because before rank 0 the precision is 100%\n",
        "      precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "\n",
        "      #add 0 to recalls because before rank 0 the recall is 0%\n",
        "      recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "\n",
        "      #get area under precision-recall graph using trapezoid rule\n",
        "      mAP = torch.trapz(precisions, recalls)\n",
        "      return mAP\n",
        "\n",
        "\n",
        "  def plot_boxes(self, predictions, true_boxes, image=None):\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "\n",
        "    height, width = 1, 1\n",
        "    if not image == None:\n",
        "      im = np.array(image)\n",
        "      print(im.shape)\n",
        "      height, width, _ = im.shape\n",
        "      ax.imshow(im)\n",
        "\n",
        "    for box in true_boxes:\n",
        "          box = box[:4]\n",
        "          upper_left_x = box[0] - box[2] / 2\n",
        "          upper_left_y = box[1] - box[3] / 2\n",
        "          rect = patches.Rectangle(\n",
        "              (upper_left_x * width, upper_left_y * height),\n",
        "              box[2] * width,\n",
        "              box[3] * height,\n",
        "              linewidth=2,\n",
        "              edgecolor=\"green\",\n",
        "              facecolor=\"none\",\n",
        "          )\n",
        "          # Add the patch to the Axes\n",
        "          ax.add_patch(rect)\n",
        "\n",
        "    for box in predictions:\n",
        "        box = box[-4:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=2,\n",
        "            edgecolor=\"blue\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  def plot_image(self, image, boxes):\n",
        "      \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "\n",
        "      im = np.array(image)\n",
        "      height, width, _ = im.shape\n",
        "  \n",
        "      # Create figure and axes\n",
        "      fig, ax = plt.subplots(1)\n",
        "      # Display the image\n",
        "      ax.imshow(im)\n",
        "  \n",
        "      # box[0] is x midpoint, box[2] is width\n",
        "      # box[1] is y midpoint, box[3] is height\n",
        "  \n",
        "      # Create a Rectangle patch\n",
        "      for box in boxes:\n",
        "          assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "          class_pred = box[0]\n",
        "          box = box[2:]\n",
        "          upper_left_x = box[0] - box[2] / 2\n",
        "          upper_left_y = box[1] - box[3] / 2\n",
        "          rect = patches.Rectangle(\n",
        "              (upper_left_x * width, upper_left_y * height),\n",
        "              box[2] * width,\n",
        "              box[3] * height,\n",
        "              linewidth=2,\n",
        "              edgecolor=\"blue\",\n",
        "              facecolor=\"none\",\n",
        "          )\n",
        "          # Add the patch to the Axes\n",
        "          ax.add_patch(rect)\n",
        "  \n",
        "      plt.show()\n",
        "  \n",
        "\n",
        "  \n",
        "  def get_evaluation_bboxes(self, \n",
        "      loader,\n",
        "      model,\n",
        "      iou_threshold = CONFIG.NMS_IOU_THRESH,\n",
        "      anchors = CONFIG.ANCHORS,\n",
        "      threshold = CONFIG.CONF_THRESHOLD,\n",
        "      box_format=\"midpoint\",\n",
        "      device=\"cuda\",\n",
        "  ):\n",
        "\n",
        "      # Make sure model is in eval before get bboxes\n",
        "      model.eval()\n",
        "      train_idx = 0\n",
        "      all_pred_boxes = []\n",
        "      all_true_boxes = []\n",
        "      for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
        "          x = x.to(CONFIG.DEVICE)\n",
        "  \n",
        "          with torch.no_grad():\n",
        "              predictions = model(x)\n",
        "  \n",
        "          batch_size = x.shape[0]\n",
        "          bboxes = [[] for _ in range(batch_size)]\n",
        "          for i in range(3):\n",
        "              S = predictions[i].shape[2]\n",
        "              anchor = torch.tensor([*anchors[i]]).to(CONFIG.DEVICE) * S\n",
        "              boxes_scale_i =self.cells_to_bboxes(\n",
        "                  predictions[i], anchor, S=S, is_preds=True\n",
        "              )\n",
        "              for idx, (box) in enumerate(boxes_scale_i):\n",
        "                  bboxes[idx] += box\n",
        "  \n",
        "          # we just want one bbox for each label, not one for each scale\n",
        "          true_bboxes = self.cells_to_bboxes(\n",
        "              labels[2], anchor, S=S, is_preds=False\n",
        "          )\n",
        "  \n",
        "          for idx in range(batch_size):\n",
        "              nms_boxes = self.non_max_suppression(\n",
        "                  bboxes[idx],\n",
        "                  iou_threshold=iou_threshold,\n",
        "                  threshold=threshold,\n",
        "                  box_format=box_format,\n",
        "              )\n",
        "  \n",
        "              for nms_box in nms_boxes:\n",
        "                  all_pred_boxes.append([train_idx] + nms_box)\n",
        "  \n",
        "              for box in true_bboxes[idx]:\n",
        "                  if box[1] > threshold:\n",
        "                      all_true_boxes.append([train_idx] + box)\n",
        "  \n",
        "              train_idx += 1\n",
        "  \n",
        "      model.train()\n",
        "      return all_pred_boxes, all_true_boxes\n",
        "\n",
        "  def get_evaluation_bboxes2(self, \n",
        "      loader,\n",
        "      model,\n",
        "      iou_threshold = CONFIG.NMS_IOU_THRESH,\n",
        "      anchors = CONFIG.ANCHORS,\n",
        "      threshold = CONFIG.CONF_THRESHOLD,\n",
        "      box_format=\"midpoint\",\n",
        "      device=\"cuda\",\n",
        "  ):\n",
        "\n",
        "      # Make sure model is in eval before get bboxes\n",
        "      model.eval()\n",
        "      train_idx = 0\n",
        "      all_pred_boxes = []\n",
        "      all_true_boxes = []\n",
        "      for batch_idx, (x, labels, true_bboxes) in enumerate(tqdm(loader)):\n",
        "          x = x.to(CONFIG.DEVICE)\n",
        "  \n",
        "          with torch.no_grad():\n",
        "              predictions = model(x)\n",
        "  \n",
        "          batch_size = x.shape[0]\n",
        "          bboxes = [[] for _ in range(batch_size)]\n",
        "          for scale_idx in range(3):\n",
        "              S = predictions[scale_idx].shape[2]\n",
        "              anchor = torch.tensor([*anchors[scale_idx]]).to(CONFIG.DEVICE) * S\n",
        "              boxes_scale_i =self.cells_to_bboxes(\n",
        "                  predictions[scale_idx], anchor, S=S, is_preds=True\n",
        "              )\n",
        "              for idx, (box) in enumerate(boxes_scale_i):\n",
        "                  bboxes[idx] += box\n",
        "  \n",
        "          # We just want one bbox for each label, not one for each scale\n",
        "          # true_bboxes = self.cells_to_bboxes(\n",
        "          #     labels[2], anchor, S=S, is_preds=False\n",
        "          # )\n",
        "\n",
        "          # true_bboxes = torch.Tensor(true_bboxes)\n",
        "          # true_bboxes = true_bboxes[true_bboxes[:,:,1] > threshold]\n",
        "          # true_bboxes.tolist()\n",
        "\n",
        "  \n",
        "          for idx in range(batch_size):\n",
        "              nms_boxes = self.non_max_suppression(\n",
        "                  bboxes[idx],\n",
        "                  iou_threshold=iou_threshold,\n",
        "                  threshold=threshold,\n",
        "                  box_format=box_format,\n",
        "              )\n",
        "  \n",
        "              all_pred_boxes.append(nms_boxes)\n",
        "              \n",
        "              true_boxes = [[box[4], 1, box[0], box[1], box[2], box[3]] for box in true_bboxes[idx].tolist()]\n",
        "              all_true_boxes.append(true_boxes)\n",
        "          \n",
        "  \n",
        "      model.train()\n",
        "      return all_pred_boxes, all_true_boxes\n",
        "\n",
        "  def plot_examples(self, \n",
        "      loader,\n",
        "      model,\n",
        "      number_of_examples = 10,\n",
        "      iou_threshold = CONFIG.NMS_IOU_THRESH,\n",
        "      anchors = CONFIG.ANCHORS,\n",
        "      threshold = CONFIG.CONF_THRESHOLD,\n",
        "      box_format=\"midpoint\",\n",
        "      device=\"cuda\",\n",
        "  ):\n",
        "\n",
        "      assert loader.batch_size == 1\n",
        "\n",
        "      # Make sure model is in eval before get bboxes\n",
        "      model.eval()\n",
        "      train_idx = 0\n",
        "      all_pred_boxes = []\n",
        "      all_true_boxes = []\n",
        "      for batch_idx, (x, labels, true_bboxes) in enumerate(loader):\n",
        "\n",
        "          print(true_bboxes.shape)\n",
        "\n",
        "          image = x.squeeze().permute(1,2,0)\n",
        "          x = x.to(CONFIG.DEVICE)\n",
        "  \n",
        "          with torch.no_grad():\n",
        "              predictions = model(x)\n",
        "  \n",
        "          bboxes = []\n",
        "          for scale_idx in range(3):\n",
        "              S = predictions[scale_idx].shape[2]\n",
        "              anchor = torch.tensor([*anchors[scale_idx]]).to(CONFIG.DEVICE) * S\n",
        "              boxes_scale_i =self.cells_to_bboxes(\n",
        "                  predictions[scale_idx], anchor, S=S, is_preds=True\n",
        "              )\n",
        "              for idx, (box) in enumerate(boxes_scale_i):\n",
        "                  bboxes += box\n",
        "\n",
        "          nms_boxes = self.non_max_suppression(\n",
        "              bboxes,\n",
        "              iou_threshold=iou_threshold,\n",
        "              threshold=threshold,\n",
        "              box_format=box_format,\n",
        "          )\n",
        "\n",
        "          UTILS.plot_boxes(nms_boxes[:50], true_bboxes.squeeze(0), image)\n",
        "\n",
        "\n",
        "  \n",
        "      model.train()\n",
        "  \n",
        "  \n",
        "  def cells_to_bboxes(self, predictions, anchors, S, is_preds=True):\n",
        "      \"\"\"\n",
        "      Scales the predictions coming from the model to\n",
        "      be relative to the entire image such that they for example later\n",
        "      can be plotted or.\n",
        "      INPUT:\n",
        "      predictions: tensor of size (N, 3, S, S, num_classes+5)\n",
        "      anchors: the anchors used for the predictions\n",
        "      S: the number of cells the image is divided in on the width (and height)\n",
        "      is_preds: whether the input is predictions or the true bounding boxes\n",
        "      OUTPUT:\n",
        "      converted_bboxes: the converted boxes of sizes (N, num_anchors * S * S, 1+5) with class index,\n",
        "                        object score, bounding box coordinates\n",
        "      \"\"\"\n",
        "      BATCH_SIZE = predictions.shape[0]\n",
        "      num_anchors = len(anchors)\n",
        "      box_predictions = predictions[..., 1:5]\n",
        "      if is_preds:\n",
        "          anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "          box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "          box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "          scores = torch.sigmoid(predictions[..., 0:1])\n",
        "          best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "      else:\n",
        "          scores = predictions[..., 0:1]\n",
        "          best_class = predictions[..., 5:6]\n",
        "  \n",
        "      cell_indices = (\n",
        "          torch.arange(S)\n",
        "          .repeat(predictions.shape[0], 3, S, 1)\n",
        "          .unsqueeze(-1)\n",
        "          .to(predictions.device)\n",
        "      )\n",
        "      x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
        "      y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "      w_h = 1 / S * box_predictions[..., 2:4]\n",
        "      converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
        "      return converted_bboxes.tolist()\n",
        "  \n",
        "  def check_class_accuracy(self, model, loader, threshold):\n",
        "      model.eval()\n",
        "      tot_class_preds, correct_class = 0, 0\n",
        "      tot_noobj, correct_noobj = 0, 0\n",
        "      tot_obj, correct_obj = 0, 0\n",
        "  \n",
        "      for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "          x = x.to(CONFIG.DEVICE)\n",
        "          with torch.no_grad():\n",
        "              out = model(x)\n",
        "  \n",
        "          for i in range(3):\n",
        "              y[i] = y[i].to(CONFIG.DEVICE)\n",
        "              obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "              noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "  \n",
        "              correct_class += torch.sum(\n",
        "                  torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "              )\n",
        "              tot_class_preds += torch.sum(obj)\n",
        "  \n",
        "              obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "              correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "              tot_obj += torch.sum(obj)\n",
        "              correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "              tot_noobj += torch.sum(noobj)\n",
        "  \n",
        "      print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
        "      print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
        "      print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
        "      model.train()\n",
        "  \n",
        "  \n",
        "  def get_mean_std(self, loader):\n",
        "      # var[X] = E[X**2] - E[X]**2\n",
        "      channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
        "  \n",
        "      for data, _ in tqdm(loader):\n",
        "          channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "          channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
        "          num_batches += 1\n",
        "  \n",
        "      mean = channels_sum / num_batches\n",
        "      std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
        "  \n",
        "      return mean, std\n",
        "  \n",
        "  \n",
        "  def save_checkpoint(self, model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "      print(\"=> Saving checkpoint\")\n",
        "      checkpoint = {\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "      }\n",
        "      torch.save(checkpoint, filename)\n",
        "  \n",
        "  \n",
        "  def load_checkpoint(self, checkpoint_file, model, optimizer, lr):\n",
        "      print(\"=> Loading checkpoint\")\n",
        "      checkpoint = torch.load(checkpoint_file, map_location=CONFIG.DEVICE)\n",
        "      model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "      optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "  \n",
        "      # If we don't do this then it will just have learning rate of old checkpoint\n",
        "      # and it will lead to many hours of debugging \\:\n",
        "      for param_group in optimizer.param_groups:\n",
        "          param_group[\"lr\"] = lr\n",
        "  \n",
        "  \n",
        "  def get_loaders(self):\n",
        "\n",
        "      train_dataset = WIDER_Dataset(\n",
        "          CONFIG.INDEX_PATH_TRAIN, \n",
        "          CONFIG.IMAGE_STORE_PATH_TRAIN, \n",
        "          transform=CONFIG.test_transforms\n",
        "      )\n",
        "\n",
        "      test_dataset = WIDER_Dataset(\n",
        "          CONFIG.INDEX_PATH_VAL, \n",
        "          CONFIG.IMAGE_STORE_PATH_VAL, \n",
        "          transform=CONFIG.test_transforms\n",
        "      )\n",
        "\n",
        "      train_dataset_validation = WIDER_Dataset(\n",
        "          CONFIG.INDEX_PATH_TRAIN, \n",
        "          CONFIG.IMAGE_STORE_PATH_TRAIN, \n",
        "          transform=CONFIG.test_transforms,\n",
        "          max_size=10\n",
        "      )\n",
        "      \n",
        "      train_loader = DataLoader(\n",
        "          dataset=train_dataset,\n",
        "          batch_size=CONFIG.BATCH_SIZE,\n",
        "          num_workers=CONFIG.NUM_WORKERS,\n",
        "          pin_memory=CONFIG.PIN_MEMORY,\n",
        "          shuffle=True,\n",
        "          drop_last=False,\n",
        "      )\n",
        "\n",
        "      test_loader = DataLoader(\n",
        "          dataset=test_dataset,\n",
        "          batch_size=CONFIG.BATCH_SIZE,\n",
        "          num_workers=CONFIG.NUM_WORKERS,\n",
        "          pin_memory=CONFIG.PIN_MEMORY,\n",
        "          shuffle=False,\n",
        "          drop_last=False,\n",
        "      )\n",
        "\n",
        "      train_loader_validation = DataLoader(\n",
        "          dataset=train_dataset_validation,\n",
        "          batch_size=1,\n",
        "          num_workers=0,\n",
        "          pin_memory=CONFIG.PIN_MEMORY,\n",
        "          shuffle=False,\n",
        "          drop_last=False,\n",
        "      )\n",
        "  \n",
        "      return train_loader, test_loader, train_loader_validation\n",
        "  \n",
        "  def plot_couple_examples(self, model, loader, thresh, iou_thresh, anchors):\n",
        "      model.eval()\n",
        "      x, y = next(iter(loader))\n",
        "      x = x.to(\"cuda\")\n",
        "      with torch.no_grad():\n",
        "          out = model(x)\n",
        "          bboxes = [[] for _ in range(x.shape[0])]\n",
        "          for i in range(3):\n",
        "              batch_size, A, S, _, _ = out[i].shape\n",
        "              anchor = anchors[i]\n",
        "              boxes_scale_i = cells_to_bboxes(\n",
        "                  out[i], anchor, S=S, is_preds=True\n",
        "              )\n",
        "              for idx, (box) in enumerate(boxes_scale_i):\n",
        "                  bboxes[idx] += box\n",
        "  \n",
        "          model.train()\n",
        "  \n",
        "      for i in range(batch_size):\n",
        "          nms_boxes = non_max_suppression(\n",
        "              bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
        "          )\n",
        "          self.plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)\n",
        "  \n",
        "  def seed_everything(self, seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "UTILS = Utils()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-i7tSfjThwV"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBN8dgJ9Tjvd"
      },
      "source": [
        "\"\"\"\n",
        "Creates a Pytorch dataset to load the WIDER face dataset\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "def clamp(n, minn, maxn):\n",
        "    return max(min(maxn, n), minn)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function returns a dictionary containing all training samples, for each sample containing the image location (\"file_name\") and defention of the bounding boxes (\"bounding_boxes\")\n",
        "returns: the above discribed dict\n",
        "\"\"\"\n",
        "def read_data_index(index_path):\n",
        "\n",
        "  training_set = []\n",
        "\n",
        "  f = open(index_path, \"r\")\n",
        "\n",
        "  while True:\n",
        "    file_name = f.readline().strip()\n",
        "\n",
        "    if not file_name: break\n",
        "\n",
        "    boxes = []\n",
        "    box_count = int(f.readline())\n",
        "    if box_count >= 1:\n",
        "      for i in range(box_count):\n",
        "        box = f.readline().strip().split(' ')\n",
        "        box = tuple([int(x) for x in box])\n",
        "        boxes.append(box)\n",
        "    else:\n",
        "        next(f)\n",
        "\n",
        "    \n",
        "    instance = {\n",
        "        \"file_name\": file_name,\n",
        "        \"bounding_boxes\": boxes\n",
        "        }\n",
        "\n",
        "    training_set.append(instance)\n",
        "\n",
        "  return training_set\n",
        "\n",
        "\n",
        "\n",
        "class WIDER_Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        index_path,\n",
        "        image_store_path,\n",
        "        transform=None,\n",
        "        max_size=None,\n",
        "        anchors=CONFIG.ANCHORS,\n",
        "        image_size=CONFIG.IMAGE_SIZE,\n",
        "        S=CONFIG.S\n",
        "    ):\n",
        "        self.index_path = index_path\n",
        "        self.image_store_path = image_store_path\n",
        "        self.data_index = read_data_index(index_path)\n",
        "        self.max_size = max_size\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
        "        self.num_anchors = self.anchors.shape[0]\n",
        "        self.num_anchors_per_scale = self.num_anchors // 3\n",
        "        self.ignore_iou_thresh = 0.5\n",
        "        self.DEFAULT_FACE_CLASS = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data_index) if not self.max_size else self.max_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        index_item = self.data_index[index]\n",
        "        bboxes = index_item.get('bounding_boxes')\n",
        "        img_path = self.image_store_path + index_item.get('file_name')\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image_width, image_height = image.size\n",
        "\n",
        "        image = np.array(image)\n",
        "\n",
        "        bboxes = [\n",
        "                  [\n",
        "                  (bbox[0] + 0.5 * bbox[2] - 1) / image_width, \n",
        "                  (bbox[1] + 0.5 * bbox[3] - 1) / image_height,\n",
        "                  (bbox[2]) / image_width,\n",
        "                  (bbox[3]) / image_height,\n",
        "                  self.DEFAULT_FACE_CLASS\n",
        "                  ]\n",
        "                  for bbox in bboxes] #reshape the index from the wider face data to that used by the YOLO v3 tutorial\n",
        "\n",
        "        if self.transform:\n",
        "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
        "            image = augmentations[\"image\"]\n",
        "            bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "\n",
        "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
        "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S] #[p_o, x, y, w, h, c]\n",
        "\n",
        "        for box in bboxes: #assign which anchor and which cell should be repsonsible for prediction of each target\n",
        "\n",
        "            iou_anchors = UTILS.iou_width_height(torch.tensor(box[2:4]), self.anchors)  #get a list of the IOU for all anchor boxes for this bounding box (only sends the width and the height from each bouding box: 'box[2:4]')\n",
        "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)  #sort the list of IOU's to get the largest values first\n",
        "\n",
        "            x, y, width, height, class_label = box\n",
        "            has_anchor = [False] * 3  # each scale should have one anchor (we want to pick one anchor per scale for each bounding box)\n",
        "\n",
        "            for anchor_idx in anchor_indices: #find an anchor that fits\n",
        "                scale_idx = anchor_idx // self.num_anchors_per_scale #determine which scale we're working with\n",
        "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale #determines the anchor number on that scale\n",
        "                S = self.S[scale_idx] #get the grid size\n",
        "                i, j = int(S * y), int(S * x)  #determine the cell within the grid cell\n",
        "\n",
        "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0] #check the objectness value for this anchor to see if its already taken by by some other bbox\n",
        "\n",
        "                if not anchor_taken and not has_anchor[scale_idx]: #make sure anchor has not been taken allready and that we don't allready have an anchor for this particular scale on this bounding box.\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1  #set objectness prediction target to 1 (as there is an object here)\n",
        "                    x_cell, y_cell = S * x - j, S * y - i  #  determine the x and y coordinates relative to the cell, both between [0,1]\n",
        "                    width_cell, height_cell = (\n",
        "                        width * S,\n",
        "                        height * S,\n",
        "                    )  #  determine the width and height of the box relative to the cell, can be greater than 1 since it's relative to cell\n",
        "\n",
        "                    #TODO: width and height should be relative to the anchor right?\n",
        "\n",
        "                    box_coordinates = torch.tensor(\n",
        "                        [x_cell, y_cell, width_cell, height_cell]\n",
        "                    )\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates #1;5 = x, y, width, height\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label) #add the class label (this can be ignored for face detection?)\n",
        "                    has_anchor[scale_idx] = True  #notify that the bbox has an anchor for this scale\n",
        "\n",
        "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh: #if there is already an anchor for this bbox (so with a better iou)\n",
        "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # set objectness of this anchor to -1 to signal to ignore prediction\n",
        "\n",
        "        return image, tuple(targets), torch.Tensor(bboxes)\n",
        "        # return image, bboxes\n",
        " \n",
        " \n",
        "# def test():\n",
        "#     anchors = CONFIG.ANCHORS\n",
        " \n",
        "#     transform = CONFIG.test_transforms\n",
        " \n",
        "#     dataset = WIDER_Dataset(\n",
        "#           CONFIG.INDEX_PATH_TRAIN, \n",
        "#           CONFIG.IMAGE_STORE_PATH_TRAIN, \n",
        "#           transform=transform\n",
        "#       )\n",
        "\n",
        "#     S = [13, 26, 52]\n",
        "#     scaled_anchors = torch.tensor(anchors) / (\n",
        "#         1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "#     )\n",
        "#     loader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
        "#     for x, y in loader:\n",
        "#         boxes = []\n",
        " \n",
        "#         for i in range(y[0].shape[1]):\n",
        "#             anchor = scaled_anchors[i]\n",
        "#             print(anchor.shape)\n",
        "#             print(y[i].shape)\n",
        "#             boxes += UTILS.cells_to_bboxes(\n",
        "#                 y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
        "#             )[0]\n",
        "#         boxes = UTILS.non_max_suppression(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
        "#         print(boxes)\n",
        "#         UTILS.plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
        " \n",
        "# test()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ehtU6UTQSW"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11nGIxVPTO4f"
      },
      "source": [
        "\"\"\"\n",
        "Implementation of Yolo Loss Function similar to the one in Yolov3 paper,\n",
        "the difference from what I can tell is I use CrossEntropy for the classes\n",
        "instead of BinaryCrossEntropy.\n",
        "\"\"\"\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.entropy = nn.CrossEntropyLoss()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Constants signifying how much to pay for each respective part of the loss\n",
        "        self.lambda_class = 0\n",
        "        self.lambda_noobj = 10\n",
        "        self.lambda_obj = 2\n",
        "        self.lambda_box = 10\n",
        "\n",
        "    def forward(self, predictions, target, anchors):\n",
        "        # Check where obj and noobj (we ignore if target == -1)\n",
        "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
        "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
        "\n",
        "        # ======================= #\n",
        "        #   FOR NO OBJECT LOSS    #\n",
        "        # ======================= #\n",
        "\n",
        "        no_object_loss = self.bce(\n",
        "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
        "        )\n",
        "\n",
        "        # ==================== #\n",
        "        #   FOR OBJECT LOSS    #\n",
        "        # ==================== #\n",
        "\n",
        "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
        "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
        "        ious = UTILS.intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
        "        object_loss = self.mse(self.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
        "\n",
        "        # ======================== #\n",
        "        #   FOR BOX COORDINATES    #\n",
        "        # ======================== #\n",
        "\n",
        "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])  # x,y coordinates\n",
        "        target[..., 3:5] = torch.log(\n",
        "            (1e-16 + target[..., 3:5] / anchors)\n",
        "        )  # width, height coordinates\n",
        "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
        "\n",
        "        # ================== #\n",
        "        #   FOR CLASS LOSS   #\n",
        "        # ================== #\n",
        "\n",
        "        class_loss = self.entropy(\n",
        "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
        "        )\n",
        "\n",
        "        #print(\"__________________________________\")\n",
        "        #print(self.lambda_box * box_loss)\n",
        "        #print(self.lambda_obj * object_loss)\n",
        "        #print(self.lambda_noobj * no_object_loss)\n",
        "        #print(self.lambda_class * class_loss)\n",
        "        #print(\"\\n\")\n",
        "\n",
        "        return (\n",
        "            self.lambda_box * box_loss\n",
        "            + self.lambda_obj * object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + self.lambda_class * class_loss\n",
        "        )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKTKFfHnTYsx"
      },
      "source": [
        "# train method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGSbObVKS9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e25c19c-f5fe-4b52-83ce-e043eb510d8b",
        "tags": []
      },
      "source": [
        "\"\"\"\n",
        "Main file for training Yolo model on WIDER Face\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "from IPython.display import clear_output\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    losses = [] \n",
        "    for batch_idx, (x, y, true_boxes) in enumerate(loop):\n",
        "        x = x.to(CONFIG.DEVICE)\n",
        "        y0, y1, y2 = ( #three different scales\n",
        "            y[0].to(CONFIG.DEVICE),\n",
        "            y[1].to(CONFIG.DEVICE),\n",
        "            y[2].to(CONFIG.DEVICE),\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast(): #float 16 in pythorch (according to video)\n",
        "            out = model(x)\n",
        "            loss = ( #loss function based on the output (out) at the three different levels\n",
        "                loss_fn(out[0], y0, scaled_anchors[0])\n",
        "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update progress bar\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        loop.set_postfix(loss=mean_loss)\n",
        "\n",
        "\n",
        "def evaluate_model(test_loader, model):\n",
        "\n",
        "    UTILS.plot_examples(test_loader, model)\n",
        "\n",
        "  # pred_boxes, true_boxes = UTILS.get_evaluation_bboxes2(\n",
        "  #               test_loader,\n",
        "  #               model\n",
        "  #           )\n",
        "  \n",
        "  # model.train()\n",
        "  \n",
        "  # mAP = UTILS.mean_average_precision3(pred_boxes, true_boxes)\n",
        "\n",
        "\n",
        "  # return mAP\n",
        "\n",
        "\n",
        "def main(model):\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=CONFIG.LEARNING_RATE, weight_decay=CONFIG.WEIGHT_DECAY\n",
        "    )\n",
        "    loss_fn = YoloLoss()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, test_loader, train_loader_evaluation = UTILS.get_loaders()\n",
        "\n",
        "    if CONFIG.LOAD_MODEL:\n",
        "        UTILS.load_checkpoint(\n",
        "            CONFIG.CHECKPOINT_FILE, model, optimizer, CONFIG.LEARNING_RATE\n",
        "        )\n",
        "\n",
        "    scaled_anchors = (\n",
        "        torch.tensor(CONFIG.ANCHORS)\n",
        "        * torch.tensor(CONFIG.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    ).to(CONFIG.DEVICE)\n",
        "\n",
        "    for epoch in range(500): #range(CONFIG.NUM_EPOCHS):\n",
        "        if epoch > 0 and epoch % 5 == 0:\n",
        "            clear_output(wait=True)\n",
        "        train_fn(train_loader_evaluation, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "\n",
        "        #if SAVE_MODEL:\n",
        "        #    UTILS.save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "\n",
        "        #print(f\"Currently epoch {epoch}\")\n",
        "        #print(\"On Train Eval loader:\")\n",
        "        #print(\"On Train loader:\")\n",
        "        #UTILS.check_class_accuracy(model, train_loader, threshold=CONFIG.CONF_THRESHOLD)\n",
        "\n",
        "        # every so many epochs, evaluate the model\n",
        "        if epoch > 0 and epoch % 10 == 0:\n",
        "            # UTILS.check_class_accuracy(model, train_loader_evaluation, threshold=CONFIG.CONF_THRESHOLD)\n",
        "\n",
        "            mAP = evaluate_model(train_loader_evaluation, model)\n",
        "            print(\"mAP = {}\".format(mAP))\n",
        "\n",
        "            \n",
        "            # \n",
        "            # mapval = UTILS.mean_average_precision(\n",
        "            #     pred_boxes,\n",
        "            #     true_boxes,\n",
        "            #     iou_threshold=CONFIG.MAP_IOU_THRESH,\n",
        "            #     box_format=\"midpoint\",\n",
        "            #     num_classes=CONFIG.NUM_CLASSES,\n",
        "            # )\n",
        "            # print(f\"MAP: {mapval.item()}\")\n",
        "            \n",
        "model = YOLOv3(num_classes=CONFIG.NUM_CLASSES).to(CONFIG.DEVICE)\n",
        "main(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [256, 15, 1, 1], expected input[1, 320, 52, 52] to have 15 channels, but got 320 channels instead",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-6-f5d9d5bc1f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYOLOv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-6-f5d9d5bc1f9f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader_evaluation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;31m#if SAVE_MODEL:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-f5d9d5bc1f9f>\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#float 16 in pythorch (according to video)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             loss = ( #loss function based on the output (out) at the three different levels\n\u001b[0;32m     32\u001b[0m                 \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_anchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-1-3a2408bc8fca>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m#count += 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;31m#print(\"count is \"+str(count))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m       \u001b[1;31m#      if isinstance(layer, ResidualBlock) and layer.num_repeats == 8: #store this layer for the feature pyramid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-1-3a2408bc8fca>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bn_act\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 395\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 15, 1, 1], expected input[1, 320, 52, 52] to have 15 channels, but got 320 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}